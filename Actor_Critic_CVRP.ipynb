{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXu1r8qvSzWf"
      },
      "source": [
        "# Actor-critic learning for the Capacitated Vehicle Routing Problem.\n",
        "## Reinforcement Learning Term Project\n",
        "\n",
        "### Melike Ã‡olak - N22239753\n",
        "\n",
        "This study successfully applies the Actor-Critic framework to the Capacitated Vehicle Routing Problem (CVRP), demonstrating its effectiveness in optimizing vehicle routes. By utilizing neural networks and attention mechanisms, the model efficiently handles spatial and temporal dependencies, leading to significant reductions in total travel distance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xjm2onHdT-Av"
      },
      "source": [
        "## Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Ikr2p0Js8iB4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "from gym import wrappers\n",
        "from torch.autograd import Variable\n",
        "from torch.distributions import Categorical\n",
        "from collections import deque"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2mkXSZ-JWOr"
      },
      "source": [
        "## Creating a CVRP environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "5A98HsVVJbT5"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "from gym import error, spaces, utils\n",
        "from gym.utils import seeding\n",
        "import numpy as np\n",
        "import random\n",
        "import copy\n",
        "import math\n",
        "\n",
        "class VRPEnv(gym.Env):\n",
        "  def __init__(self):\n",
        "    # customer count ('0' is depot)\n",
        "    self.customer_count = 11\n",
        "    # the capacity of vehicles\n",
        "    self.vehicle_capacity = 2\n",
        "\n",
        "    self.action_space = spaces.Discrete(3)\n",
        "    self.observation_space = spaces.Box(low=0,high=1, shape=(4,1), dtype=np.float64)\n",
        "    self.VRP = np.array((self.customer_count,4))\n",
        "    self._max_episode_steps = 1000\n",
        "    self.viewer = None\n",
        "    self.state = None\n",
        "    self.steps_beyond_done = None\n",
        "    self.route = []\n",
        "    self.route.append(0)\n",
        "    self.previous_action = 0\n",
        "    self.hn_actor = torch.zeros([1,self.customer_count,128], dtype=torch.float32).to(device)\n",
        "    self.hn_actor_target = torch.zeros([1,self.customer_count,128], dtype=torch.float32).to(device)\n",
        "\n",
        "\n",
        "  def reset(self, seed=200):\n",
        "    if seed == 200:\n",
        "      seed = int(time.time())\n",
        "    np.random.seed(seed)\n",
        "    x_locations = (np.random.rand(self.customer_count)).reshape((self.customer_count,1))\n",
        "    y_locations = (np.random.rand(self.customer_count)).reshape((self.customer_count,1))\n",
        "    demand = (np.random.randint(1,9,self.customer_count).reshape((self.customer_count,1))).reshape((self.customer_count,1))/10 # Normalise to between 0.1 and 0.9\n",
        "    capacity = np.repeat(self.vehicle_capacity,self.customer_count).reshape((self.customer_count,1))\n",
        "    VRP = np.concatenate((np.concatenate((np.concatenate((x_locations,y_locations), axis=1),demand),axis=1),capacity),axis=1)\n",
        "    self.VRP = VRP.reshape((self.customer_count,4))\n",
        "    self.unserved_customers = []\n",
        "    for i in range(1, self.customer_count):\n",
        "      self.unserved_customers.append(i)\n",
        "    self.routes = []\n",
        "    self.route = []\n",
        "    self.route.append(0)\n",
        "    self.VRP[0,2] = 0 # Set the demand at thedepot to 0\n",
        "    self.state = copy.deepcopy(self.VRP)\n",
        "    self.previous_action = 0\n",
        "    return self.state\n",
        "\n",
        "\n",
        "  def step(self, action):\n",
        "    # Calculate the reward as the negative euclidean distance\n",
        "    reward = -((self.state[self.previous_action,0]-self.state[action,0])**2+(self.state[self.previous_action,1]-self.state[action,1])**2)**0.5 # - Euclidean distance between customers\n",
        "    load = self.state[0,3]\n",
        "    self.state[:,3] = max(0,(load-self.state[action,2])) # Update the vehicle load\n",
        "    self.state[action, 2] = max(0,self.state[action,2]-load) # Update the demand at served customer\n",
        "    done = False\n",
        "    if action == 0: # Return to the depot\n",
        "      self.route.append(action) # End route\n",
        "      self.routes.append(self.route) # Add subroute to list of all routes\n",
        "      self.route = [] # Initiate new subroute\n",
        "      self.state[:,3] = self.vehicle_capacity # Refill the vehicle\n",
        "    self.route.append(action) # Add action to the subroute\n",
        "    if max(self.state[:,2]) > 0: # If there are unserved customers left\n",
        "      done = False\n",
        "    elif max(self.state[:,2]) == 0 and action == 0: # If there are no unserved customers left and we have returned to the depot\n",
        "      done = True\n",
        "      self.route.append(0)\n",
        "    self.previous_action = action # Update the previous action\n",
        "    return self.state, reward, done\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kyy2xkZRNCRw"
      },
      "source": [
        "## Let's test the environment step function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_xu5wGbNINA",
        "outputId": "9f1a1e31-b7e6-47c4-e1a3-55046bbd4052"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.70908939 0.91106942 0.         2.        ]\n",
            " [0.12045592 0.60507709 0.6        2.        ]\n",
            " [0.84684195 0.95226466 0.5        2.        ]\n",
            " [0.84908922 0.60897652 0.7        2.        ]\n",
            " [0.12772312 0.62772292 0.4        2.        ]\n",
            " [0.31782774 0.11996001 0.8        2.        ]\n",
            " [0.29251975 0.66194704 0.4        2.        ]\n",
            " [0.45576082 0.90551701 0.4        2.        ]\n",
            " [0.40244439 0.35044002 0.8        2.        ]\n",
            " [0.04629414 0.86490402 0.4        2.        ]\n",
            " [0.19556882 0.22917711 0.4        2.        ]]\n"
          ]
        }
      ],
      "source": [
        "env = VRPEnv() # Create an instance of the environment\n",
        "state = env.reset() # Reset the environment\n",
        "action = 2 # Perform action with customer 2\n",
        "print(state)\n",
        "state, reward, done = env.step(action) # Perform the actual transition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2nGdtlKVydr"
      },
      "source": [
        "## Initialize the Experience Replay memory\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "u5rW0IDB8nTO"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer(object):\n",
        "\n",
        "  def __init__(self, max_size=1e6):\n",
        "    self.storage = []\n",
        "    self.max_size = max_size\n",
        "    self.ptr = 0\n",
        "\n",
        "  def add(self, transition):\n",
        "    if len(self.storage) == self.max_size:\n",
        "      #self.storage[int(self.ptr)] = transition\n",
        "      #self.ptr = (self.ptr + 1) % self.max_size\n",
        "      self.storage.pop(0)\n",
        "      self.storage.append(transition)\n",
        "    else:\n",
        "      self.storage.append(transition)\n",
        "\n",
        "  def sample(self, batch_size):\n",
        "    ind =  np.arange((len(self.storage)-(batch_size+1)),len(self.storage)-1,1)\n",
        "    #np.random.randint(0, len(self.storage), size=batch_size)\n",
        "    batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = [], [], [], [], []\n",
        "    for i in ind:\n",
        "      state, next_state, action, reward, done = self.storage[i]\n",
        "      batch_states.append(np.array(state, copy=False))\n",
        "      batch_next_states.append(np.array(next_state, copy=False))\n",
        "      batch_actions.append(np.array(action, copy=False))\n",
        "      batch_rewards.append(np.array(reward, copy=False))\n",
        "      batch_dones.append(np.array(done, copy=False))\n",
        "    return np.array(batch_states), np.array(batch_next_states), np.array(batch_actions), np.array(batch_rewards).reshape(-1, 1), np.array(batch_dones).reshape(-1, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jb7TTaHxWbQD"
      },
      "source": [
        "## Build the neural network for the Actor and Actor-target models - contains the attention mechanism"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "4CeRW4D79HL0"
      },
      "outputs": [],
      "source": [
        "class Actor(nn.Module):\n",
        "\n",
        "  def __init__(self, state_dim=4, embed_size = 128):#, action_dim, max_action):\n",
        "    super(Actor, self).__init__()\n",
        "    self.embed = nn.Linear((state_dim), embed_size) # Encoding to higher dimensional space - can also be changed to convolutional layer as in the paper\n",
        "    self.u_t = nn.RNN(embed_size,embed_size,1) # RNN Layer for the attention mechanism\n",
        "    self.v_t_a = nn.Linear(embed_size,1) # Linear for getting u_t\n",
        "    self.bar_u_t = nn.RNN(embed_size,embed_size,1) # RNN Layer for the context vector\n",
        "    self.a_t = nn.Softmax(dim = 1) # Softmax layer for the attention mechanism\n",
        "    self.v_t_u = nn.Linear(embed_size,1) # Linear for getting u_t\n",
        "    self.final = nn.Softmax(dim = 1) # Softmax layer for the final output\n",
        "\n",
        "    self.saved_log_probabilities = [] # Create and empty list for saving the log probabilities of the actions\n",
        "    self.rewards = [] # Create an empty list for the rewards\n",
        "    self.dones = [] # Create an empty list for checking whether the episode was completed\n",
        "\n",
        "  def forward(self, x, hn = env.hn_actor):\n",
        "    cond1 = (x[:,:,2]<x[:,:,3]).int() # Can we meet the demand\n",
        "    cond2 = (x[:,:,2]>0).int() # Is there demand at the customer\n",
        "    mask1 = torch.minimum(cond1,cond2) # Select those customers with demand, and whose demand we can meet\n",
        "    if torch.sum(mask1[:,1:len(mask1[0])]) == 0: # If only the depot can be visited\n",
        "      mask1[:,0] = 1\n",
        "    mask1 = torch.reshape(mask1,(len(x),env.customer_count,1))\n",
        "    x = self.embed(x)\n",
        "    u, hn = self.u_t(x, hn)\n",
        "    u = self.v_t_a(u)\n",
        "    a = self.a_t(u)\n",
        "    c = torch.randn(x.shape)\n",
        "    c = torch.mul(x,a)\n",
        "    c = torch.sum(c, 0)\n",
        "    c = torch.reshape(c,(1,env.customer_count,128))\n",
        "    u_bar, hu = self.bar_u_t(x,c)\n",
        "    u_bar = self.v_t_u(u_bar)\n",
        "    output = self.final(u_bar)\n",
        "    output = torch.mul(output,mask1)\n",
        "    #print(\"Before clamp \", output)\n",
        "    #output = output.log().clamp(epsilon, 1 - epsilon)\n",
        "    epsilon = 10 ** -44\n",
        "    output = output.clamp(min=1e-4)\n",
        "    #print(\"After clamp \",output)\n",
        "    output = output.log()#.clamp(epsilon, 1 - epsilon)\n",
        "    #print(\"After log \",output)\n",
        "    output = self.final(output)\n",
        "    return output\n",
        "\n",
        "class Actor_Target(nn.Module):\n",
        "\n",
        "  def __init__(self, state_dim=4, embed_size = 128):#, action_dim, max_action):\n",
        "    super(Actor_Target, self).__init__()\n",
        "    self.embed = nn.Linear((state_dim), embed_size) # Encoding to higher dimensional space - can also be changed to convolutional layer as in the paper\n",
        "    self.u_t = nn.RNN(embed_size,embed_size,1) # RNN Layer for the attention mechanism\n",
        "    self.v_t_a = nn.Linear(embed_size,1) # Linear for getting u_t\n",
        "    self.bar_u_t = nn.RNN(embed_size,embed_size,1) # RNN Layer for the context vector\n",
        "    self.a_t = nn.Softmax(dim = 1) # Softmax layer for the attention mechanism\n",
        "    self.v_t_u = nn.Linear(embed_size,1) # Linear for getting u_t\n",
        "    self.final = nn.Softmax(dim = 1) # Softmax layer for the final output\n",
        "\n",
        "  def forward(self, x, hn = env.hn_actor_target):\n",
        "    cond1 = (x[:,:,2]<x[:,:,3]).int() # Can we meet the demand\n",
        "    cond2 = (x[:,:,2]>0).int() # Is there demand at the customer\n",
        "    mask1 = torch.minimum(cond1,cond2) # Select those customers with demand, and whose demand we can meet\n",
        "    if torch.sum(mask1[:,1:len(mask1[0])]) == 0: # If only the depot can be visited\n",
        "      mask1[:,0] = 1\n",
        "    mask1 = torch.reshape(mask1,(len(x),env.customer_count,1))\n",
        "    x = self.embed(x)\n",
        "    u, hn = self.u_t(x, hn)\n",
        "    u = self.v_t_a(u)\n",
        "    a = self.a_t(u) # Up to equation (4) now\n",
        "    c = torch.randn(x.shape)\n",
        "    c = torch.mul(x,a)\n",
        "    c = torch.sum(c, 0)\n",
        "    c = torch.reshape(c,(1,env.customer_count,128))\n",
        "    u_bar, hu = self.bar_u_t(x,c)\n",
        "    u_bar = self.v_t_u(u_bar)\n",
        "    output = self.final(u_bar)\n",
        "    output = torch.mul(output,mask1)\n",
        "    output = self.final(torch.log(output))\n",
        "    return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDRPWgEVNRcF"
      },
      "source": [
        "## Build the neural network for the Critic and Critic-target model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "_z9ifdVdNRLO"
      },
      "outputs": [],
      "source": [
        "class Critic(nn.Module):\n",
        "\n",
        "  def __init__(self, state_dim=4, action_dim = env.customer_count, embed_size = 128):\n",
        "    super(Critic, self).__init__()\n",
        "    # Defining the first Critic neural network\n",
        "    self.layer_1 = nn.Linear(state_dim, embed_size) # Perform the embedding\n",
        "    self.layer_2 = nn.Linear(embed_size, embed_size) # Adding the single dense layer\n",
        "    self.layer_3 = nn.Linear(embed_size, 1) # Adding the output layer\n",
        "\n",
        "    self.values = [] # Create an empty list to store the predicted values\n",
        "\n",
        "\n",
        "  def forward(self, x, u): # x is the state, u is the action\n",
        "    # Forward-Propagation on the Critic Neural Network\n",
        "    x1 = F.relu(self.layer_1(x))\n",
        "    ws = torch.mul(x1,u)\n",
        "    x2 = F.relu(self.layer_2(ws))\n",
        "    x2 = self.layer_3(x2)\n",
        "    x2 = torch.sum(x2,1)\n",
        "    return x2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VmkiLbHonzy"
      },
      "source": [
        "## Testing the actor and critic networks to confirm their output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sQ_-V52dw76O",
        "outputId": "468b9edc-255e-4e55-a397-cfb7c051412e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[0.0001],\n",
            "         [0.0945],\n",
            "         [0.1029],\n",
            "         [0.0983],\n",
            "         [0.1004],\n",
            "         [0.0966],\n",
            "         [0.1009],\n",
            "         [0.0983],\n",
            "         [0.1041],\n",
            "         [0.0985],\n",
            "         [0.1053]]], grad_fn=<SoftmaxBackward0>)\n",
            "tensor([[0.7240]], grad_fn=<SumBackward1>)\n",
            "tensor(1., grad_fn=<SumBackward0>)\n"
          ]
        }
      ],
      "source": [
        "env = VRPEnv()\n",
        "env.reset()\n",
        "state = env.reset()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "state = torch.Tensor(state.reshape(1,env.customer_count,4)).to(device)\n",
        "actor = Actor().to(device)\n",
        "actor_target = Actor_Target().to(device)\n",
        "prediction = actor(state)\n",
        "prediction_target = actor_target(state)\n",
        "print(prediction)\n",
        "critic = Critic().to(device)\n",
        "q_value = critic(state,prediction)\n",
        "print(q_value)\n",
        "\n",
        "state, reward, done = env.step(5)\n",
        "state = torch.Tensor(state.reshape(1,env.customer_count,4)).to(device)\n",
        "prediction = actor(state)\n",
        "print(torch.sum(prediction))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzIDuONodenW"
      },
      "source": [
        "## Training Process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "zzd0H1xukdKe"
      },
      "outputs": [],
      "source": [
        "# Selecting the device (CPU or GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.autograd.set_detect_anomaly(True)\n",
        "\n",
        "# Building the Training Process into a class\n",
        "class Actor_Critic(object):\n",
        "\n",
        "  def __init__(self, state_dim):\n",
        "    self.actor = Actor_Target(state_dim).to(device)\n",
        "    self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr = 0.0001)\n",
        "    self.critic = Critic(state_dim, action_dim).to(device)\n",
        "    self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr = 0.0001)\n",
        "    self.max_action = max_action\n",
        "\n",
        "  def select_action(self, state, state_dim):\n",
        "    #state_tensor = torch.Tensor(state.reshape(1,env.customer_count,state_dim)).to(device)\n",
        "    current_Q = actor(state)\n",
        "    Q = torch.reshape(current_Q,(1,env.customer_count))\n",
        "    m = Categorical(Q)\n",
        "    action = m.sample()\n",
        "    actor.saved_log_probabilities.append(m.log_prob(action))\n",
        "    return action.item(), current_Q\n",
        "\n",
        "  def select_target_action(self, state, state_dim):\n",
        "    #state_tensor = torch.Tensor(state.reshape(1,env.customer_count,state_dim)).to(device)\n",
        "    target_Q = actor(state)\n",
        "    Q = target_Q.detach().cpu().numpy().reshape(env.customer_count)\n",
        "    action = np.argmax(Q)\n",
        "    return action, target_Q\n",
        "\n",
        "  def train(self, iterations, batch_size=32, tau=0.005):\n",
        "    for it in range(iterations):\n",
        "      running_reward = 8\n",
        "\n",
        "      # Ensuring that the variables have been cleared\n",
        "      del actor.rewards[:]\n",
        "      del actor.saved_log_probabilities[:]\n",
        "      del critic.values[:]\n",
        "\n",
        "      returns = [] # Create an empty list for the returns for the batch of episodes\n",
        "      Steps = [] # Create an empty list for the number of steps required for each episode\n",
        "      steps = 0\n",
        "      for b in range(batch_size):\n",
        "        # Working with the actor and the critic\n",
        "        obs = env.reset(((it+1)*b))\n",
        "        obs = torch.Tensor(obs.reshape(1,env.customer_count,state_dim)).to(device)\n",
        "        InstantReward = 0\n",
        "        done = False\n",
        "        Steps.append(steps)\n",
        "        steps = 0\n",
        "        while not done:\n",
        "          # Complete episode playing actions as selected by the actor\n",
        "          action, current_Q = self.select_action(obs, state_dim)\n",
        "          obs, reward, done = env.step(action)\n",
        "          InstantReward += reward\n",
        "          actor.rewards.append(reward)\n",
        "          actor.dones.append(done)\n",
        "\n",
        "          obs = torch.Tensor(obs.reshape(1,env.customer_count,state_dim)).to(device)\n",
        "\n",
        "          # Predict the expected value of the action with the critic\n",
        "          predicted_value = critic(obs, current_Q)\n",
        "          critic.values.append(predicted_value)\n",
        "          steps += 1\n",
        "        running_reward = 0.05 * InstantReward + (1 - 0.05) * running_reward\n",
        "        #print(env.routes)\n",
        "      # Let's finish the batch by updating the return\n",
        "      R = 0\n",
        "      for i in range(len(actor.rewards)):\n",
        "        if actor.dones[len(actor.rewards)-1-i] == True:\n",
        "          R = 0\n",
        "        R = actor.rewards[len(actor.rewards)-1-i] + discount*R\n",
        "        returns.insert(0,R)\n",
        "\n",
        "      returns = torch.tensor(returns)\n",
        "      returns = (returns - torch.tensor(critic.values))\n",
        "\n",
        "      actor_loss = []\n",
        "      critic_loss = []\n",
        "      # Let's update the models\n",
        "      for log_prob, R in zip(actor.saved_log_probabilities, returns):\n",
        "        actor_loss.append(-log_prob * R)\n",
        "      self.actor_optimizer.zero_grad()\n",
        "      actor_loss = torch.cat(actor_loss).mean()\n",
        "      actor_loss.backward(retain_graph=True)\n",
        "      self.actor_optimizer.step()\n",
        "      del actor.rewards[:]\n",
        "      del actor.saved_log_probabilities[:]\n",
        "\n",
        "      for values, R in zip(critic.values, returns):\n",
        "        critic_loss.append((R-values)**2)\n",
        "      self.critic_optimizer.zero_grad()\n",
        "      critic_loss = torch.cat(critic_loss).mean()\n",
        "      critic_loss.backward(retain_graph=True)\n",
        "      self.critic_optimizer.step()\n",
        "      del critic.values[:]\n",
        "\n",
        "\n",
        "      print(\"Batch {} completed, Last reward: {:.2f} Average reward: {:.2f}\".format(it, InstantReward, running_reward))\n",
        "\n",
        "  # Make a save method to save a trained model\n",
        "  def save(self, filename, directory):\n",
        "    torch.save(self.actor.state_dict(), '%s/%s_actor.pth' % (directory, filename))\n",
        "    torch.save(self.critic.state_dict(), '%s/%s_actor_baseline.pth' % (directory, filename))\n",
        "\n",
        "  # Making a load method to load a pre-trained model\n",
        "  def load(self, filename, directory):\n",
        "    self.actor.load_state_dict(torch.load('%s/%s_actor.pth' % (directory, filename)))\n",
        "    self.critic.load_state_dict(torch.load('%s/%s_actor_baseline.pth' % (directory, filename)))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bnyr1rrq2qIH",
        "outputId": "c5e3f901-1326-4a27-fc01-4d2db50596cb"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ka-ZRtQvjBex"
      },
      "source": [
        "## Create a function that evaluates the policy by calculating its average reward over 10 episodes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "qabqiYdp9wDM"
      },
      "outputs": [],
      "source": [
        "def evaluate_policy(policy, eval_episodes=10):\n",
        "  avg_reward = 0.\n",
        "  for _ in range(eval_episodes):\n",
        "    obs = env.reset()\n",
        "    obs = torch.Tensor(obs.reshape(1,env.customer_count,state_dim)).to(device)\n",
        "    done = False\n",
        "    while not done:\n",
        "      action, current_Q = policy.select_target_action(obs, state_dim)\n",
        "      obs, reward, done = env.step(action)\n",
        "      obs = torch.Tensor(obs.reshape(1,env.customer_count,state_dim)).to(device)\n",
        "      avg_reward += reward\n",
        "  avg_reward /= eval_episodes\n",
        "  print (\"---------------------------------------\")\n",
        "  print (\"Average Reward over the Evaluation Step: %f\" % (avg_reward))\n",
        "  print (\"---------------------------------------\")\n",
        "  return avg_reward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGuKmH_ijf7U"
      },
      "source": [
        "# Set the parameters\n",
        "## There are my experiments on different parameter combinations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "HFj6wbAo97lk"
      },
      "outputs": [],
      "source": [
        "env_name = \"CVRP\" # Name of a environment\n",
        "seed = 0 # Random seed number\n",
        "start_timesteps = 1e4 # Number of iterations/timesteps before which the model randomly chooses an action, and after which it starts to use the policy network\n",
        "eval_freq = 5e3 # How often the evaluation step is performed (after how many timesteps)\n",
        "max_timesteps = 1000 # Total number of iterations/timesteps\n",
        "save_models = True # Boolean checker whether or not to save the pre-trained model\n",
        "batch_size = 64 # Size of the batch\n",
        "discount = 0.99 # Discount factor gamma, used in the calculation of the total discounted reward\n",
        "tau = 0.001 # Target network update rate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env_name = \"CVRP_deneme2\"\n",
        "seed = 0\n",
        "start_timesteps = 1e4\n",
        "eval_freq = 5e3\n",
        "max_timesteps = 10\n",
        "save_models = True\n",
        "batch_size = 128\n",
        "discount = 0.99\n",
        "tau = 0.001"
      ],
      "metadata": {
        "id": "2DdvNcBS4EU6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env_name = \"CVRP_deneme3\"\n",
        "seed = 0\n",
        "start_timesteps = 1e4\n",
        "eval_freq = 5e3\n",
        "max_timesteps = 10\n",
        "save_models = True\n",
        "batch_size = 16\n",
        "discount = 0.99\n",
        "tau = 0.001"
      ],
      "metadata": {
        "id": "2h93kKFVgX0a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env_name = \"CVRP_deneme4\"\n",
        "seed = 0\n",
        "start_timesteps = 1e4\n",
        "eval_freq = 5e3\n",
        "max_timesteps = 10\n",
        "save_models = True\n",
        "batch_size = 16\n",
        "discount = 0.99\n",
        "tau = 0.001"
      ],
      "metadata": {
        "id": "lGFXeSDvEQ0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hjwf2HCol3XP"
      },
      "source": [
        "## Create a file name for the two saved models: the Actor and Critic models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1fyH8N5z-o3o",
        "outputId": "4c279d07-ada4-4fd8-b4ec-f8a12adbacae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------------\n",
            "Settings: Actor_Critic_CVRP_0\n",
            "---------------------------------------\n"
          ]
        }
      ],
      "source": [
        "file_name = \"%s_%s_%s\" % (\"Actor_Critic\", env_name, str(seed))\n",
        "print (\"---------------------------------------\")\n",
        "print (\"Settings: %s\" % (file_name))\n",
        "\n",
        "print (\"---------------------------------------\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kop-C96Aml8O"
      },
      "source": [
        "## Create a folder to save the trained models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0k9Iq__GBfAE",
        "outputId": "bd8ab186-86a3-4f37-a371-d78f06d10cec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/REINFORCEMENT LEARNING\n"
          ]
        }
      ],
      "source": [
        "cd \"/content/drive/MyDrive/REINFORCEMENT LEARNING\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Src07lvY-zXb"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(\"./results\"):\n",
        "  os.makedirs(\"./results\")\n",
        "if save_models and not os.path.exists(\"./pytorch_models\"):\n",
        "  os.makedirs(\"./pytorch_models\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEAzOd47mv1Z"
      },
      "source": [
        ":## Create an instance of the CVRP environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "CyQXJUIs-6BV"
      },
      "outputs": [],
      "source": [
        "env = VRPEnv()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YdPG4HXnNsh"
      },
      "source": [
        "## Set seeds and get the necessary information on the states and actions in the chosen environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z3RufYec_ADj",
        "outputId": "d7db4b52-c0d9-4662-a0ef-3b560b3069fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:256: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ],
      "source": [
        "env.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.customer_count\n",
        "max_action = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWEgDAQxnbem"
      },
      "source": [
        "## Create the policy network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wTVvG7F8_EWg",
        "outputId": "6598e781-c37d-4df6-92d3-53b79be9d6f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4 11 1\n"
          ]
        }
      ],
      "source": [
        "print(state_dim, action_dim, max_action)\n",
        "policy = Actor_Critic(state_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38ohC4HSwEVT",
        "outputId": "b0d93cbe-a4c0-4ccc-db22-0155bdb284c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4 11 1\n"
          ]
        }
      ],
      "source": [
        "print(state_dim, action_dim, max_action)\n",
        "policy = Actor_Critic(state_dim)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZI60VN2Unklh"
      },
      "source": [
        "## Create the Experience Replay memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "sd-ZsdXR_LgV"
      },
      "outputs": [],
      "source": [
        "replay_buffer = ReplayBuffer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYOpCyiDnw7s"
      },
      "source": [
        "## Define a list where all the evaluation results over 10 episodes are stored"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dhC_5XJ__Orp",
        "outputId": "81413586-1dcc-4c57-8905-92396ddd4375"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: -7.297477\n",
            "---------------------------------------\n",
            "[[0, 3, 1, 5, 10, 7, 9, 6, 0], [0, 4, 2, 8, 0]]\n"
          ]
        }
      ],
      "source": [
        "evaluations = [evaluate_policy(policy, eval_episodes=1)]\n",
        "print(env.routes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-JT8iQRwMJJ",
        "outputId": "2bcc1e1c-a8de-416c-ff8a-81147d77791b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: -6.568879\n",
            "---------------------------------------\n",
            "[[0, 7, 8, 1, 2, 6, 0], [0, 4, 10, 3, 5, 0], [0, 9, 0]]\n"
          ]
        }
      ],
      "source": [
        "evaluations = [evaluate_policy(policy, eval_episodes=1)]\n",
        "print(env.routes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6t3LV3qZaWkt",
        "outputId": "3f42a769-9c10-4016-cc98-b6c7e0de9a73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0, 3, 1, 5, 10, 7, 9, 6, 0], [0, 4, 2, 8, 0]]\n",
            "[[0.98649204 0.3908403  0.         2.        ]\n",
            " [0.15493414 0.42325573 0.         2.        ]\n",
            " [0.92832032 0.83326726 0.         2.        ]\n",
            " [0.31530229 0.77515289 0.         2.        ]\n",
            " [0.38922382 0.08818484 0.         2.        ]\n",
            " [0.70459249 0.94054456 0.         2.        ]\n",
            " [0.72482578 0.6617625  0.         2.        ]\n",
            " [0.81498624 0.83440967 0.         2.        ]\n",
            " [0.61663679 0.02821797 0.         2.        ]\n",
            " [0.18864103 0.05720503 0.         2.        ]\n",
            " [0.73024361 0.80385569 0.         2.        ]]\n"
          ]
        }
      ],
      "source": [
        "print(env.routes)\n",
        "print(env.state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t6ToJUC2Br6t",
        "outputId": "39ceb4b2-979a-48ae-e00d-1fc7fd14680e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/REINFORCEMENT LEARNING\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "cd \"/content/drive/MyDrive/REINFORCEMENT LEARNING\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G8nj8cA_BwbW",
        "outputId": "cf5632ea-1341-4592-8b88-c6be267897c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mexp\u001b[0m/  \u001b[01;34mpytorch_models\u001b[0m/  ReinforceAttempt_V1.ipynb  \u001b[01;34mresults\u001b[0m/\n"
          ]
        }
      ],
      "source": [
        "ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xm-4b3p6rglE"
      },
      "source": [
        "## Create a folder directory in which the final results will be saved"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "MTL9uMd0ru03"
      },
      "outputs": [],
      "source": [
        "def mkdir(base, name):\n",
        "    path = os.path.join(base, name)\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "    return path\n",
        "work_dir = mkdir('exp', 'brs')\n",
        "monitor_dir = mkdir(work_dir, 'monitor')\n",
        "max_episode_steps = env._max_episode_steps\n",
        "save_env_vid = False\n",
        "if save_env_vid:\n",
        "  env = wrappers.Monitor(env, monitor_dir, force = True)\n",
        "  env.reset()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31n5eb03p-Fm"
      },
      "source": [
        "## Initialize the training process variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "1vN5EvxK_QhT"
      },
      "outputs": [],
      "source": [
        "total_timesteps = 0\n",
        "timesteps_since_eval = 0\n",
        "episode_num = 0\n",
        "done = True\n",
        "t0 = time.time()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9gsjvtPqLgT"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We start the main loop over 500,000 timesteps\n",
        "\n",
        "\n",
        "env_name = \"CVRP\" # Name of a environment (set it to any Continous environment you want)\n",
        "seed = 0 # Random seed number\n",
        "start_timesteps = 1e4 # Number of iterations/timesteps before which the model randomly chooses an action, and after which it starts to use the policy network\n",
        "eval_freq = 5e3 # How often the evaluation step is performed (after how many timesteps)\n",
        "max_timesteps = 10 # Total number of iterations/timesteps\n",
        "save_models = True # Boolean checker whether or not to save the pre-trained model\n",
        "batch_size = 128 # Size of the batch\n",
        "discount = 0.99 # Discount factor gamma, used in the calculation of the total discounted reward\n",
        "tau = 0.001 # Target network update rate\n",
        "\n",
        "env.reset()\n",
        "total_timesteps = 0\n",
        "obs = copy.deepcopy(env.reset())\n",
        "\n",
        "policy.train(int(max_timesteps), batch_size, tau)\n",
        "\n",
        "# Add the last policy evaluation to our list of evaluations and we save our model\n",
        "evaluations.append(evaluate_policy(policy))\n",
        "if save_models: policy.save(\"%s\" % (file_name), directory=\"./pytorch_models\")\n",
        "np.save(\"./results/%s\" % (file_name), evaluations)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Spj-eZVTg1CS",
        "outputId": "468db6db-7074-4b41-b667-407054e78a7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 0 completed, Last reward: -7.25 Average reward: -6.88\n",
            "Batch 1 completed, Last reward: -6.03 Average reward: -6.86\n",
            "Batch 2 completed, Last reward: -4.52 Average reward: -6.65\n",
            "Batch 3 completed, Last reward: -6.12 Average reward: -6.68\n",
            "Batch 4 completed, Last reward: -8.67 Average reward: -6.81\n",
            "Batch 5 completed, Last reward: -7.82 Average reward: -6.99\n",
            "Batch 6 completed, Last reward: -6.24 Average reward: -6.59\n",
            "Batch 7 completed, Last reward: -6.59 Average reward: -6.96\n",
            "Batch 8 completed, Last reward: -8.35 Average reward: -6.93\n",
            "Batch 9 completed, Last reward: -6.32 Average reward: -6.64\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: -5.427587\n",
            "---------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_ouY4NH_Y0I",
        "outputId": "0addd2b6-9b57-497f-827a-1e951af04e87"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 0 completed, Last reward: -7.37 Average reward: -6.79\n",
            "Batch 1 completed, Last reward: -6.71 Average reward: -6.82\n",
            "Batch 2 completed, Last reward: -5.70 Average reward: -6.82\n",
            "Batch 3 completed, Last reward: -6.52 Average reward: -6.53\n",
            "Batch 4 completed, Last reward: -7.07 Average reward: -6.55\n",
            "Batch 5 completed, Last reward: -7.01 Average reward: -6.74\n",
            "Batch 6 completed, Last reward: -4.88 Average reward: -6.68\n",
            "Batch 7 completed, Last reward: -5.24 Average reward: -6.71\n",
            "Batch 8 completed, Last reward: -7.61 Average reward: -6.83\n",
            "Batch 9 completed, Last reward: -6.70 Average reward: -6.70\n",
            "Batch 10 completed, Last reward: -6.85 Average reward: -6.68\n",
            "Batch 11 completed, Last reward: -8.05 Average reward: -6.89\n",
            "Batch 12 completed, Last reward: -6.94 Average reward: -6.94\n",
            "Batch 13 completed, Last reward: -7.41 Average reward: -6.37\n",
            "Batch 14 completed, Last reward: -5.64 Average reward: -6.98\n",
            "Batch 15 completed, Last reward: -7.74 Average reward: -6.56\n",
            "Batch 16 completed, Last reward: -7.12 Average reward: -6.83\n",
            "Batch 17 completed, Last reward: -5.08 Average reward: -6.61\n",
            "Batch 18 completed, Last reward: -6.01 Average reward: -6.67\n",
            "Batch 19 completed, Last reward: -7.54 Average reward: -6.83\n",
            "Batch 20 completed, Last reward: -9.71 Average reward: -6.92\n",
            "Batch 21 completed, Last reward: -7.12 Average reward: -6.59\n",
            "Batch 22 completed, Last reward: -5.33 Average reward: -6.83\n",
            "Batch 23 completed, Last reward: -4.89 Average reward: -6.40\n",
            "Batch 24 completed, Last reward: -5.57 Average reward: -6.54\n",
            "Batch 25 completed, Last reward: -6.32 Average reward: -6.68\n",
            "Batch 26 completed, Last reward: -6.51 Average reward: -6.83\n",
            "Batch 27 completed, Last reward: -7.43 Average reward: -6.58\n",
            "Batch 28 completed, Last reward: -4.62 Average reward: -6.50\n",
            "Batch 29 completed, Last reward: -6.46 Average reward: -6.74\n",
            "Batch 30 completed, Last reward: -6.02 Average reward: -6.79\n",
            "Batch 31 completed, Last reward: -6.35 Average reward: -6.77\n",
            "Batch 32 completed, Last reward: -5.92 Average reward: -6.47\n",
            "Batch 33 completed, Last reward: -6.77 Average reward: -6.42\n",
            "Batch 34 completed, Last reward: -5.89 Average reward: -6.42\n",
            "Batch 35 completed, Last reward: -7.14 Average reward: -6.67\n",
            "Batch 36 completed, Last reward: -6.32 Average reward: -6.89\n",
            "Batch 37 completed, Last reward: -6.33 Average reward: -6.85\n",
            "Batch 38 completed, Last reward: -6.00 Average reward: -6.77\n",
            "Batch 39 completed, Last reward: -3.74 Average reward: -6.62\n",
            "Batch 40 completed, Last reward: -5.60 Average reward: -6.73\n",
            "Batch 41 completed, Last reward: -5.40 Average reward: -6.48\n",
            "Batch 42 completed, Last reward: -6.35 Average reward: -6.50\n",
            "Batch 43 completed, Last reward: -6.86 Average reward: -7.04\n",
            "Batch 44 completed, Last reward: -6.10 Average reward: -6.67\n",
            "Batch 45 completed, Last reward: -7.94 Average reward: -6.68\n",
            "Batch 46 completed, Last reward: -8.51 Average reward: -6.61\n",
            "Batch 47 completed, Last reward: -6.45 Average reward: -7.08\n",
            "Batch 48 completed, Last reward: -5.94 Average reward: -6.91\n",
            "Batch 49 completed, Last reward: -7.40 Average reward: -6.67\n",
            "Batch 50 completed, Last reward: -7.05 Average reward: -6.41\n",
            "Batch 51 completed, Last reward: -5.77 Average reward: -6.44\n",
            "Batch 52 completed, Last reward: -6.24 Average reward: -6.66\n",
            "Batch 53 completed, Last reward: -6.45 Average reward: -6.84\n",
            "Batch 54 completed, Last reward: -5.49 Average reward: -6.77\n",
            "Batch 55 completed, Last reward: -7.89 Average reward: -7.13\n",
            "Batch 56 completed, Last reward: -7.34 Average reward: -6.83\n",
            "Batch 57 completed, Last reward: -5.05 Average reward: -6.89\n",
            "Batch 58 completed, Last reward: -6.10 Average reward: -6.93\n",
            "Batch 59 completed, Last reward: -6.93 Average reward: -6.80\n",
            "Batch 60 completed, Last reward: -5.64 Average reward: -6.73\n",
            "Batch 61 completed, Last reward: -7.56 Average reward: -6.87\n",
            "Batch 62 completed, Last reward: -9.32 Average reward: -7.24\n",
            "Batch 63 completed, Last reward: -9.15 Average reward: -6.97\n",
            "Batch 64 completed, Last reward: -7.03 Average reward: -6.90\n",
            "Batch 65 completed, Last reward: -6.87 Average reward: -6.81\n",
            "Batch 66 completed, Last reward: -5.05 Average reward: -6.78\n",
            "Batch 67 completed, Last reward: -6.71 Average reward: -6.64\n",
            "Batch 68 completed, Last reward: -6.14 Average reward: -7.05\n",
            "Batch 69 completed, Last reward: -6.81 Average reward: -7.06\n",
            "Batch 70 completed, Last reward: -6.27 Average reward: -6.38\n",
            "Batch 71 completed, Last reward: -6.23 Average reward: -6.67\n",
            "Batch 72 completed, Last reward: -7.71 Average reward: -6.96\n",
            "Batch 73 completed, Last reward: -6.85 Average reward: -6.77\n",
            "Batch 74 completed, Last reward: -7.55 Average reward: -7.02\n",
            "Batch 75 completed, Last reward: -6.00 Average reward: -6.50\n",
            "Batch 76 completed, Last reward: -10.87 Average reward: -6.78\n",
            "Batch 77 completed, Last reward: -6.98 Average reward: -6.78\n",
            "Batch 78 completed, Last reward: -6.18 Average reward: -6.83\n",
            "Batch 79 completed, Last reward: -7.68 Average reward: -6.92\n",
            "Batch 80 completed, Last reward: -8.17 Average reward: -6.60\n",
            "Batch 81 completed, Last reward: -6.91 Average reward: -6.90\n",
            "Batch 82 completed, Last reward: -7.53 Average reward: -6.86\n",
            "Batch 83 completed, Last reward: -6.30 Average reward: -6.68\n",
            "Batch 84 completed, Last reward: -6.55 Average reward: -6.69\n",
            "Batch 85 completed, Last reward: -5.77 Average reward: -6.75\n",
            "Batch 86 completed, Last reward: -7.80 Average reward: -6.65\n",
            "Batch 87 completed, Last reward: -7.60 Average reward: -7.08\n",
            "Batch 88 completed, Last reward: -8.57 Average reward: -6.68\n",
            "Batch 89 completed, Last reward: -9.70 Average reward: -6.64\n",
            "Batch 90 completed, Last reward: -7.63 Average reward: -6.94\n",
            "Batch 91 completed, Last reward: -5.89 Average reward: -6.75\n",
            "Batch 92 completed, Last reward: -5.58 Average reward: -6.58\n",
            "Batch 93 completed, Last reward: -8.30 Average reward: -6.76\n",
            "Batch 94 completed, Last reward: -5.72 Average reward: -6.82\n",
            "Batch 95 completed, Last reward: -7.56 Average reward: -6.54\n",
            "Batch 96 completed, Last reward: -6.69 Average reward: -6.91\n",
            "Batch 97 completed, Last reward: -6.05 Average reward: -6.80\n",
            "Batch 98 completed, Last reward: -6.41 Average reward: -6.93\n",
            "Batch 99 completed, Last reward: -6.54 Average reward: -6.89\n",
            "Batch 100 completed, Last reward: -6.36 Average reward: -6.77\n",
            "Batch 101 completed, Last reward: -9.77 Average reward: -7.07\n",
            "Batch 102 completed, Last reward: -5.14 Average reward: -6.88\n",
            "Batch 103 completed, Last reward: -6.20 Average reward: -6.73\n",
            "Batch 104 completed, Last reward: -5.80 Average reward: -6.40\n",
            "Batch 105 completed, Last reward: -4.73 Average reward: -6.98\n",
            "Batch 106 completed, Last reward: -5.84 Average reward: -7.04\n",
            "Batch 107 completed, Last reward: -6.49 Average reward: -7.04\n",
            "Batch 108 completed, Last reward: -6.49 Average reward: -6.52\n",
            "Batch 109 completed, Last reward: -5.93 Average reward: -6.96\n",
            "Batch 110 completed, Last reward: -6.37 Average reward: -6.56\n",
            "Batch 111 completed, Last reward: -6.31 Average reward: -6.70\n",
            "Batch 112 completed, Last reward: -7.78 Average reward: -6.94\n",
            "Batch 113 completed, Last reward: -8.07 Average reward: -6.88\n",
            "Batch 114 completed, Last reward: -6.35 Average reward: -6.70\n",
            "Batch 115 completed, Last reward: -4.97 Average reward: -7.02\n",
            "Batch 116 completed, Last reward: -7.92 Average reward: -6.79\n",
            "Batch 117 completed, Last reward: -8.56 Average reward: -6.87\n",
            "Batch 118 completed, Last reward: -4.99 Average reward: -6.46\n",
            "Batch 119 completed, Last reward: -6.41 Average reward: -7.02\n",
            "Batch 120 completed, Last reward: -6.49 Average reward: -6.89\n",
            "Batch 121 completed, Last reward: -7.42 Average reward: -6.70\n",
            "Batch 122 completed, Last reward: -6.32 Average reward: -6.93\n",
            "Batch 123 completed, Last reward: -7.13 Average reward: -6.87\n",
            "Batch 124 completed, Last reward: -6.15 Average reward: -7.03\n",
            "Batch 125 completed, Last reward: -5.95 Average reward: -6.72\n",
            "Batch 126 completed, Last reward: -6.83 Average reward: -6.90\n",
            "Batch 127 completed, Last reward: -9.79 Average reward: -6.91\n",
            "Batch 128 completed, Last reward: -7.00 Average reward: -6.98\n",
            "Batch 129 completed, Last reward: -5.69 Average reward: -6.37\n",
            "Batch 130 completed, Last reward: -6.63 Average reward: -6.60\n",
            "Batch 131 completed, Last reward: -8.16 Average reward: -6.89\n",
            "Batch 132 completed, Last reward: -7.06 Average reward: -6.75\n",
            "Batch 133 completed, Last reward: -6.64 Average reward: -6.75\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 134 completed, Last reward: -7.12 Average reward: -6.82\n",
            "Batch 135 completed, Last reward: -5.41 Average reward: -6.83\n",
            "Batch 136 completed, Last reward: -5.06 Average reward: -6.55\n",
            "Batch 137 completed, Last reward: -8.91 Average reward: -6.94\n",
            "Batch 138 completed, Last reward: -5.25 Average reward: -7.01\n",
            "Batch 139 completed, Last reward: -6.82 Average reward: -6.66\n",
            "Batch 140 completed, Last reward: -7.51 Average reward: -6.72\n",
            "Batch 141 completed, Last reward: -4.83 Average reward: -6.66\n",
            "Batch 142 completed, Last reward: -8.98 Average reward: -6.63\n",
            "Batch 143 completed, Last reward: -7.07 Average reward: -6.61\n",
            "Batch 144 completed, Last reward: -5.91 Average reward: -6.75\n",
            "Batch 145 completed, Last reward: -5.62 Average reward: -6.53\n",
            "Batch 146 completed, Last reward: -6.19 Average reward: -7.00\n",
            "Batch 147 completed, Last reward: -6.42 Average reward: -6.50\n",
            "Batch 148 completed, Last reward: -6.67 Average reward: -6.88\n",
            "Batch 149 completed, Last reward: -7.50 Average reward: -6.72\n",
            "Batch 150 completed, Last reward: -3.79 Average reward: -6.62\n",
            "Batch 151 completed, Last reward: -7.69 Average reward: -6.59\n",
            "Batch 152 completed, Last reward: -6.23 Average reward: -6.70\n",
            "Batch 153 completed, Last reward: -7.24 Average reward: -6.82\n",
            "Batch 154 completed, Last reward: -5.68 Average reward: -6.94\n",
            "Batch 155 completed, Last reward: -6.22 Average reward: -6.74\n",
            "Batch 156 completed, Last reward: -6.52 Average reward: -6.59\n",
            "Batch 157 completed, Last reward: -9.90 Average reward: -6.74\n",
            "Batch 158 completed, Last reward: -6.41 Average reward: -6.77\n",
            "Batch 159 completed, Last reward: -8.27 Average reward: -6.70\n",
            "Batch 160 completed, Last reward: -6.06 Average reward: -6.64\n",
            "Batch 161 completed, Last reward: -6.76 Average reward: -6.89\n",
            "Batch 162 completed, Last reward: -6.51 Average reward: -6.85\n",
            "Batch 163 completed, Last reward: -8.73 Average reward: -6.89\n",
            "Batch 164 completed, Last reward: -6.73 Average reward: -6.77\n",
            "Batch 165 completed, Last reward: -8.45 Average reward: -6.62\n",
            "Batch 166 completed, Last reward: -7.00 Average reward: -6.71\n",
            "Batch 167 completed, Last reward: -6.79 Average reward: -6.63\n",
            "Batch 168 completed, Last reward: -6.10 Average reward: -6.82\n",
            "Batch 169 completed, Last reward: -5.01 Average reward: -6.80\n",
            "Batch 170 completed, Last reward: -8.59 Average reward: -6.71\n",
            "Batch 171 completed, Last reward: -6.38 Average reward: -6.70\n",
            "Batch 172 completed, Last reward: -7.27 Average reward: -6.44\n",
            "Batch 173 completed, Last reward: -6.80 Average reward: -6.63\n",
            "Batch 174 completed, Last reward: -6.26 Average reward: -6.74\n",
            "Batch 175 completed, Last reward: -8.86 Average reward: -6.83\n",
            "Batch 176 completed, Last reward: -5.18 Average reward: -6.61\n",
            "Batch 177 completed, Last reward: -7.19 Average reward: -6.42\n",
            "Batch 178 completed, Last reward: -7.27 Average reward: -6.53\n",
            "Batch 179 completed, Last reward: -7.60 Average reward: -6.72\n",
            "Batch 180 completed, Last reward: -6.62 Average reward: -6.58\n",
            "Batch 181 completed, Last reward: -6.63 Average reward: -7.01\n",
            "Batch 182 completed, Last reward: -6.84 Average reward: -6.62\n",
            "Batch 183 completed, Last reward: -7.29 Average reward: -6.65\n",
            "Batch 184 completed, Last reward: -8.20 Average reward: -6.93\n",
            "Batch 185 completed, Last reward: -6.21 Average reward: -7.00\n",
            "Batch 186 completed, Last reward: -8.49 Average reward: -6.72\n",
            "Batch 187 completed, Last reward: -7.51 Average reward: -7.10\n",
            "Batch 188 completed, Last reward: -7.68 Average reward: -7.12\n",
            "Batch 189 completed, Last reward: -4.67 Average reward: -6.86\n",
            "Batch 190 completed, Last reward: -4.15 Average reward: -6.68\n",
            "Batch 191 completed, Last reward: -5.56 Average reward: -6.50\n",
            "Batch 192 completed, Last reward: -6.51 Average reward: -6.80\n",
            "Batch 193 completed, Last reward: -5.61 Average reward: -6.60\n",
            "Batch 194 completed, Last reward: -6.05 Average reward: -6.79\n",
            "Batch 195 completed, Last reward: -6.50 Average reward: -6.79\n",
            "Batch 196 completed, Last reward: -8.28 Average reward: -7.01\n",
            "Batch 197 completed, Last reward: -6.35 Average reward: -6.68\n",
            "Batch 198 completed, Last reward: -8.36 Average reward: -6.66\n",
            "Batch 199 completed, Last reward: -4.91 Average reward: -6.63\n",
            "Batch 200 completed, Last reward: -6.01 Average reward: -6.67\n",
            "Batch 201 completed, Last reward: -6.81 Average reward: -6.96\n",
            "Batch 202 completed, Last reward: -6.71 Average reward: -6.89\n",
            "Batch 203 completed, Last reward: -7.10 Average reward: -6.59\n",
            "Batch 204 completed, Last reward: -7.80 Average reward: -6.78\n",
            "Batch 205 completed, Last reward: -7.26 Average reward: -6.70\n",
            "Batch 206 completed, Last reward: -7.46 Average reward: -6.78\n",
            "Batch 207 completed, Last reward: -7.03 Average reward: -6.54\n",
            "Batch 208 completed, Last reward: -6.58 Average reward: -6.80\n",
            "Batch 209 completed, Last reward: -5.65 Average reward: -6.91\n",
            "Batch 210 completed, Last reward: -6.80 Average reward: -7.01\n",
            "Batch 211 completed, Last reward: -4.97 Average reward: -6.94\n",
            "Batch 212 completed, Last reward: -6.39 Average reward: -6.54\n",
            "Batch 213 completed, Last reward: -6.92 Average reward: -6.53\n",
            "Batch 214 completed, Last reward: -7.90 Average reward: -6.98\n",
            "Batch 215 completed, Last reward: -8.17 Average reward: -6.69\n",
            "Batch 216 completed, Last reward: -6.04 Average reward: -6.55\n",
            "Batch 217 completed, Last reward: -6.94 Average reward: -6.71\n",
            "Batch 218 completed, Last reward: -6.59 Average reward: -6.54\n",
            "Batch 219 completed, Last reward: -5.26 Average reward: -6.64\n",
            "Batch 220 completed, Last reward: -6.81 Average reward: -7.20\n",
            "Batch 221 completed, Last reward: -6.20 Average reward: -6.97\n",
            "Batch 222 completed, Last reward: -7.38 Average reward: -6.85\n",
            "Batch 223 completed, Last reward: -7.18 Average reward: -6.52\n",
            "Batch 224 completed, Last reward: -8.55 Average reward: -6.85\n",
            "Batch 225 completed, Last reward: -8.47 Average reward: -6.98\n",
            "Batch 226 completed, Last reward: -5.30 Average reward: -6.84\n",
            "Batch 227 completed, Last reward: -4.32 Average reward: -6.85\n",
            "Batch 228 completed, Last reward: -7.60 Average reward: -6.83\n",
            "Batch 229 completed, Last reward: -6.01 Average reward: -6.78\n",
            "Batch 230 completed, Last reward: -6.75 Average reward: -6.49\n",
            "Batch 231 completed, Last reward: -8.94 Average reward: -6.88\n",
            "Batch 232 completed, Last reward: -7.78 Average reward: -6.97\n",
            "Batch 233 completed, Last reward: -5.96 Average reward: -6.75\n",
            "Batch 234 completed, Last reward: -5.42 Average reward: -6.79\n",
            "Batch 235 completed, Last reward: -7.69 Average reward: -6.86\n",
            "Batch 236 completed, Last reward: -8.58 Average reward: -7.03\n",
            "Batch 237 completed, Last reward: -7.50 Average reward: -6.83\n",
            "Batch 238 completed, Last reward: -7.42 Average reward: -6.59\n",
            "Batch 239 completed, Last reward: -6.56 Average reward: -6.88\n",
            "Batch 240 completed, Last reward: -7.78 Average reward: -6.69\n",
            "Batch 241 completed, Last reward: -7.16 Average reward: -6.74\n",
            "Batch 242 completed, Last reward: -5.26 Average reward: -6.90\n",
            "Batch 243 completed, Last reward: -6.81 Average reward: -6.70\n",
            "Batch 244 completed, Last reward: -6.71 Average reward: -7.11\n",
            "Batch 245 completed, Last reward: -7.94 Average reward: -6.57\n",
            "Batch 246 completed, Last reward: -8.18 Average reward: -6.89\n",
            "Batch 247 completed, Last reward: -5.27 Average reward: -6.48\n",
            "Batch 248 completed, Last reward: -5.39 Average reward: -6.33\n",
            "Batch 249 completed, Last reward: -5.88 Average reward: -6.73\n",
            "Batch 250 completed, Last reward: -7.54 Average reward: -6.84\n",
            "Batch 251 completed, Last reward: -7.02 Average reward: -6.83\n",
            "Batch 252 completed, Last reward: -6.23 Average reward: -6.62\n",
            "Batch 253 completed, Last reward: -5.78 Average reward: -6.43\n",
            "Batch 254 completed, Last reward: -5.31 Average reward: -6.66\n",
            "Batch 255 completed, Last reward: -5.81 Average reward: -6.52\n",
            "Batch 256 completed, Last reward: -6.78 Average reward: -6.66\n",
            "Batch 257 completed, Last reward: -6.42 Average reward: -6.65\n",
            "Batch 258 completed, Last reward: -6.40 Average reward: -6.79\n",
            "Batch 259 completed, Last reward: -7.72 Average reward: -6.76\n",
            "Batch 260 completed, Last reward: -6.90 Average reward: -6.82\n",
            "Batch 261 completed, Last reward: -5.83 Average reward: -6.85\n",
            "Batch 262 completed, Last reward: -6.12 Average reward: -6.59\n",
            "Batch 263 completed, Last reward: -8.73 Average reward: -7.04\n",
            "Batch 264 completed, Last reward: -7.44 Average reward: -6.89\n",
            "Batch 265 completed, Last reward: -6.99 Average reward: -6.81\n",
            "Batch 266 completed, Last reward: -6.89 Average reward: -6.49\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 267 completed, Last reward: -7.31 Average reward: -6.74\n",
            "Batch 268 completed, Last reward: -6.42 Average reward: -6.65\n",
            "Batch 269 completed, Last reward: -7.59 Average reward: -7.22\n",
            "Batch 270 completed, Last reward: -7.36 Average reward: -6.85\n",
            "Batch 271 completed, Last reward: -4.72 Average reward: -6.84\n",
            "Batch 272 completed, Last reward: -5.46 Average reward: -6.59\n",
            "Batch 273 completed, Last reward: -7.11 Average reward: -6.63\n",
            "Batch 274 completed, Last reward: -6.05 Average reward: -6.70\n",
            "Batch 275 completed, Last reward: -7.46 Average reward: -6.66\n",
            "Batch 276 completed, Last reward: -6.52 Average reward: -6.93\n",
            "Batch 277 completed, Last reward: -5.85 Average reward: -6.76\n",
            "Batch 278 completed, Last reward: -5.42 Average reward: -6.79\n",
            "Batch 279 completed, Last reward: -7.08 Average reward: -6.66\n",
            "Batch 280 completed, Last reward: -7.55 Average reward: -6.55\n",
            "Batch 281 completed, Last reward: -6.33 Average reward: -7.09\n",
            "Batch 282 completed, Last reward: -6.26 Average reward: -6.64\n",
            "Batch 283 completed, Last reward: -6.67 Average reward: -6.77\n",
            "Batch 284 completed, Last reward: -6.32 Average reward: -7.17\n",
            "Batch 285 completed, Last reward: -7.96 Average reward: -7.00\n",
            "Batch 286 completed, Last reward: -4.76 Average reward: -6.33\n",
            "Batch 287 completed, Last reward: -7.70 Average reward: -6.82\n",
            "Batch 288 completed, Last reward: -7.94 Average reward: -6.60\n",
            "Batch 289 completed, Last reward: -6.76 Average reward: -6.74\n",
            "Batch 290 completed, Last reward: -6.17 Average reward: -6.72\n",
            "Batch 291 completed, Last reward: -7.10 Average reward: -6.71\n",
            "Batch 292 completed, Last reward: -8.97 Average reward: -7.02\n",
            "Batch 293 completed, Last reward: -6.68 Average reward: -6.82\n",
            "Batch 294 completed, Last reward: -7.53 Average reward: -6.99\n",
            "Batch 295 completed, Last reward: -8.78 Average reward: -6.72\n",
            "Batch 296 completed, Last reward: -7.92 Average reward: -6.59\n",
            "Batch 297 completed, Last reward: -5.16 Average reward: -6.48\n",
            "Batch 298 completed, Last reward: -6.63 Average reward: -6.95\n",
            "Batch 299 completed, Last reward: -8.17 Average reward: -6.70\n",
            "Batch 300 completed, Last reward: -9.31 Average reward: -6.63\n",
            "Batch 301 completed, Last reward: -5.71 Average reward: -6.51\n",
            "Batch 302 completed, Last reward: -6.46 Average reward: -6.83\n",
            "Batch 303 completed, Last reward: -5.79 Average reward: -6.72\n",
            "Batch 304 completed, Last reward: -8.15 Average reward: -6.87\n",
            "Batch 305 completed, Last reward: -9.52 Average reward: -7.24\n",
            "Batch 306 completed, Last reward: -6.64 Average reward: -6.73\n",
            "Batch 307 completed, Last reward: -8.18 Average reward: -6.85\n",
            "Batch 308 completed, Last reward: -7.37 Average reward: -6.78\n",
            "Batch 309 completed, Last reward: -5.21 Average reward: -6.51\n",
            "Batch 310 completed, Last reward: -6.75 Average reward: -6.70\n",
            "Batch 311 completed, Last reward: -5.95 Average reward: -6.81\n",
            "Batch 312 completed, Last reward: -6.00 Average reward: -6.66\n",
            "Batch 313 completed, Last reward: -8.34 Average reward: -6.94\n",
            "Batch 314 completed, Last reward: -5.40 Average reward: -6.83\n",
            "Batch 315 completed, Last reward: -5.12 Average reward: -6.53\n",
            "Batch 316 completed, Last reward: -6.41 Average reward: -6.72\n",
            "Batch 317 completed, Last reward: -5.37 Average reward: -6.91\n",
            "Batch 318 completed, Last reward: -6.85 Average reward: -6.86\n",
            "Batch 319 completed, Last reward: -5.62 Average reward: -6.63\n",
            "Batch 320 completed, Last reward: -5.70 Average reward: -6.88\n",
            "Batch 321 completed, Last reward: -6.64 Average reward: -6.83\n",
            "Batch 322 completed, Last reward: -5.21 Average reward: -6.76\n",
            "Batch 323 completed, Last reward: -4.84 Average reward: -6.45\n",
            "Batch 324 completed, Last reward: -6.00 Average reward: -6.44\n",
            "Batch 325 completed, Last reward: -7.41 Average reward: -6.60\n",
            "Batch 326 completed, Last reward: -5.87 Average reward: -6.70\n",
            "Batch 327 completed, Last reward: -6.40 Average reward: -6.82\n",
            "Batch 328 completed, Last reward: -7.29 Average reward: -6.63\n",
            "Batch 329 completed, Last reward: -5.60 Average reward: -6.77\n",
            "Batch 330 completed, Last reward: -6.04 Average reward: -6.93\n",
            "Batch 331 completed, Last reward: -7.97 Average reward: -6.48\n",
            "Batch 332 completed, Last reward: -5.90 Average reward: -6.61\n",
            "Batch 333 completed, Last reward: -6.41 Average reward: -6.62\n",
            "Batch 334 completed, Last reward: -7.76 Average reward: -6.65\n",
            "Batch 335 completed, Last reward: -5.76 Average reward: -6.97\n",
            "Batch 336 completed, Last reward: -7.54 Average reward: -6.94\n",
            "Batch 337 completed, Last reward: -7.12 Average reward: -6.83\n",
            "Batch 338 completed, Last reward: -7.79 Average reward: -6.82\n",
            "Batch 339 completed, Last reward: -7.54 Average reward: -6.52\n",
            "Batch 340 completed, Last reward: -6.86 Average reward: -6.41\n",
            "Batch 341 completed, Last reward: -4.65 Average reward: -6.71\n",
            "Batch 342 completed, Last reward: -6.82 Average reward: -6.75\n",
            "Batch 343 completed, Last reward: -6.57 Average reward: -6.68\n",
            "Batch 344 completed, Last reward: -4.89 Average reward: -6.59\n",
            "Batch 345 completed, Last reward: -5.34 Average reward: -6.88\n",
            "Batch 346 completed, Last reward: -7.80 Average reward: -6.68\n",
            "Batch 347 completed, Last reward: -5.70 Average reward: -6.99\n",
            "Batch 348 completed, Last reward: -7.07 Average reward: -6.82\n",
            "Batch 349 completed, Last reward: -6.39 Average reward: -6.68\n",
            "Batch 350 completed, Last reward: -7.76 Average reward: -6.72\n",
            "Batch 351 completed, Last reward: -7.00 Average reward: -6.70\n",
            "Batch 352 completed, Last reward: -6.30 Average reward: -6.87\n",
            "Batch 353 completed, Last reward: -6.16 Average reward: -6.59\n",
            "Batch 354 completed, Last reward: -6.88 Average reward: -6.78\n",
            "Batch 355 completed, Last reward: -7.04 Average reward: -6.53\n",
            "Batch 356 completed, Last reward: -6.21 Average reward: -6.37\n",
            "Batch 357 completed, Last reward: -7.48 Average reward: -6.70\n",
            "Batch 358 completed, Last reward: -7.97 Average reward: -6.74\n",
            "Batch 359 completed, Last reward: -6.73 Average reward: -7.03\n",
            "Batch 360 completed, Last reward: -5.90 Average reward: -6.56\n",
            "Batch 361 completed, Last reward: -7.10 Average reward: -6.65\n",
            "Batch 362 completed, Last reward: -8.66 Average reward: -6.95\n",
            "Batch 363 completed, Last reward: -6.20 Average reward: -6.89\n",
            "Batch 364 completed, Last reward: -4.63 Average reward: -6.31\n",
            "Batch 365 completed, Last reward: -6.66 Average reward: -6.70\n",
            "Batch 366 completed, Last reward: -6.02 Average reward: -6.71\n",
            "Batch 367 completed, Last reward: -5.70 Average reward: -6.76\n",
            "Batch 368 completed, Last reward: -9.26 Average reward: -6.99\n",
            "Batch 369 completed, Last reward: -6.72 Average reward: -6.77\n",
            "Batch 370 completed, Last reward: -7.21 Average reward: -6.93\n",
            "Batch 371 completed, Last reward: -5.86 Average reward: -6.33\n",
            "Batch 372 completed, Last reward: -5.71 Average reward: -6.80\n",
            "Batch 373 completed, Last reward: -5.87 Average reward: -6.75\n",
            "Batch 374 completed, Last reward: -8.63 Average reward: -6.40\n",
            "Batch 375 completed, Last reward: -8.27 Average reward: -6.56\n",
            "Batch 376 completed, Last reward: -7.12 Average reward: -6.59\n",
            "Batch 377 completed, Last reward: -5.83 Average reward: -6.68\n",
            "Batch 378 completed, Last reward: -6.72 Average reward: -6.64\n",
            "Batch 379 completed, Last reward: -7.57 Average reward: -6.64\n",
            "Batch 380 completed, Last reward: -6.67 Average reward: -6.66\n",
            "Batch 381 completed, Last reward: -6.53 Average reward: -6.70\n",
            "Batch 382 completed, Last reward: -5.81 Average reward: -6.93\n",
            "Batch 383 completed, Last reward: -9.15 Average reward: -6.80\n",
            "Batch 384 completed, Last reward: -6.10 Average reward: -6.88\n",
            "Batch 385 completed, Last reward: -5.41 Average reward: -6.39\n",
            "Batch 386 completed, Last reward: -4.93 Average reward: -6.47\n",
            "Batch 387 completed, Last reward: -7.51 Average reward: -6.87\n",
            "Batch 388 completed, Last reward: -8.02 Average reward: -7.20\n",
            "Batch 389 completed, Last reward: -6.65 Average reward: -6.95\n",
            "Batch 390 completed, Last reward: -6.65 Average reward: -6.56\n",
            "Batch 391 completed, Last reward: -7.02 Average reward: -6.88\n",
            "Batch 392 completed, Last reward: -7.36 Average reward: -6.76\n",
            "Batch 393 completed, Last reward: -4.30 Average reward: -6.33\n",
            "Batch 394 completed, Last reward: -7.73 Average reward: -6.92\n",
            "Batch 395 completed, Last reward: -6.05 Average reward: -6.57\n",
            "Batch 396 completed, Last reward: -7.96 Average reward: -6.87\n",
            "Batch 397 completed, Last reward: -6.46 Average reward: -6.99\n",
            "Batch 398 completed, Last reward: -6.75 Average reward: -6.99\n",
            "Batch 399 completed, Last reward: -6.71 Average reward: -6.33\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 400 completed, Last reward: -7.81 Average reward: -6.85\n",
            "Batch 401 completed, Last reward: -7.16 Average reward: -6.52\n",
            "Batch 402 completed, Last reward: -6.42 Average reward: -6.65\n",
            "Batch 403 completed, Last reward: -6.09 Average reward: -6.64\n",
            "Batch 404 completed, Last reward: -7.12 Average reward: -6.58\n",
            "Batch 405 completed, Last reward: -6.13 Average reward: -6.61\n",
            "Batch 406 completed, Last reward: -9.49 Average reward: -7.26\n",
            "Batch 407 completed, Last reward: -4.63 Average reward: -6.64\n",
            "Batch 408 completed, Last reward: -4.56 Average reward: -6.76\n",
            "Batch 409 completed, Last reward: -7.73 Average reward: -6.53\n",
            "Batch 410 completed, Last reward: -5.86 Average reward: -6.56\n",
            "Batch 411 completed, Last reward: -5.37 Average reward: -6.73\n",
            "Batch 412 completed, Last reward: -7.33 Average reward: -6.69\n",
            "Batch 413 completed, Last reward: -7.84 Average reward: -6.79\n",
            "Batch 414 completed, Last reward: -4.44 Average reward: -6.41\n",
            "Batch 415 completed, Last reward: -7.57 Average reward: -6.89\n",
            "Batch 416 completed, Last reward: -9.21 Average reward: -6.89\n",
            "Batch 417 completed, Last reward: -6.06 Average reward: -6.78\n",
            "Batch 418 completed, Last reward: -6.27 Average reward: -6.75\n",
            "Batch 419 completed, Last reward: -6.48 Average reward: -6.78\n",
            "Batch 420 completed, Last reward: -5.03 Average reward: -6.58\n",
            "Batch 421 completed, Last reward: -7.65 Average reward: -6.57\n",
            "Batch 422 completed, Last reward: -8.61 Average reward: -6.61\n",
            "Batch 423 completed, Last reward: -6.26 Average reward: -6.78\n",
            "Batch 424 completed, Last reward: -7.05 Average reward: -6.80\n",
            "Batch 425 completed, Last reward: -4.74 Average reward: -6.75\n",
            "Batch 426 completed, Last reward: -6.38 Average reward: -7.03\n",
            "Batch 427 completed, Last reward: -8.13 Average reward: -6.91\n",
            "Batch 428 completed, Last reward: -8.30 Average reward: -6.94\n",
            "Batch 429 completed, Last reward: -5.96 Average reward: -6.47\n",
            "Batch 430 completed, Last reward: -6.93 Average reward: -6.55\n",
            "Batch 431 completed, Last reward: -8.03 Average reward: -6.67\n"
          ]
        }
      ],
      "source": [
        "# We start the main loop over 500,000 timesteps\n",
        "env.reset()\n",
        "total_timesteps = 0\n",
        "obs = copy.deepcopy(env.reset())\n",
        "\n",
        "policy.train(int(max_timesteps), batch_size, tau)\n",
        "\n",
        "# Add the last policy evaluation to our list of evaluations and we save our model\n",
        "evaluations.append(evaluate_policy(policy))\n",
        "if save_models: policy.save(\"%s\" % (file_name), directory=\"./pytorch_models\")\n",
        "np.save(\"./results/%s\" % (file_name), evaluations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F7pyKctxwPc0",
        "outputId": "8c0cedf4-27a3-436c-b602-a7bc3cb74ee4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 0 completed, Last reward: -7.25 Average reward: -6.87\n",
            "Batch 1 completed, Last reward: -6.61 Average reward: -6.76\n",
            "Batch 2 completed, Last reward: -4.52 Average reward: -6.67\n",
            "Batch 3 completed, Last reward: -6.78 Average reward: -6.56\n",
            "Batch 4 completed, Last reward: -8.67 Average reward: -6.73\n",
            "Batch 5 completed, Last reward: -7.82 Average reward: -6.98\n",
            "Batch 6 completed, Last reward: -6.36 Average reward: -6.66\n",
            "Batch 7 completed, Last reward: -6.59 Average reward: -7.00\n",
            "Batch 8 completed, Last reward: -7.78 Average reward: -6.81\n",
            "Batch 9 completed, Last reward: -6.32 Average reward: -6.60\n",
            "Batch 10 completed, Last reward: -6.21 Average reward: -6.71\n",
            "Batch 11 completed, Last reward: -7.19 Average reward: -6.78\n",
            "Batch 12 completed, Last reward: -7.90 Average reward: -6.97\n",
            "Batch 13 completed, Last reward: -8.61 Average reward: -6.72\n",
            "Batch 14 completed, Last reward: -5.94 Average reward: -7.01\n",
            "Batch 15 completed, Last reward: -7.31 Average reward: -6.45\n",
            "Batch 16 completed, Last reward: -6.92 Average reward: -6.76\n",
            "Batch 17 completed, Last reward: -4.18 Average reward: -6.57\n",
            "Batch 18 completed, Last reward: -7.14 Average reward: -6.35\n",
            "Batch 19 completed, Last reward: -8.48 Average reward: -6.84\n",
            "Batch 20 completed, Last reward: -10.04 Average reward: -7.09\n",
            "Batch 21 completed, Last reward: -6.43 Average reward: -6.67\n",
            "Batch 22 completed, Last reward: -5.58 Average reward: -6.74\n",
            "Batch 23 completed, Last reward: -5.16 Average reward: -6.51\n",
            "Batch 24 completed, Last reward: -6.21 Average reward: -6.73\n",
            "Batch 25 completed, Last reward: -7.80 Average reward: -6.83\n",
            "Batch 26 completed, Last reward: -5.93 Average reward: -6.65\n",
            "Batch 27 completed, Last reward: -8.95 Average reward: -6.60\n",
            "Batch 28 completed, Last reward: -5.84 Average reward: -6.70\n",
            "Batch 29 completed, Last reward: -6.27 Average reward: -6.57\n",
            "Batch 30 completed, Last reward: -6.43 Average reward: -6.74\n",
            "Batch 31 completed, Last reward: -7.35 Average reward: -6.68\n",
            "Batch 32 completed, Last reward: -5.33 Average reward: -6.58\n",
            "Batch 33 completed, Last reward: -6.72 Average reward: -6.57\n",
            "Batch 34 completed, Last reward: -7.70 Average reward: -6.58\n",
            "Batch 35 completed, Last reward: -6.38 Average reward: -6.98\n",
            "Batch 36 completed, Last reward: -7.45 Average reward: -6.82\n",
            "Batch 37 completed, Last reward: -6.06 Average reward: -6.77\n",
            "Batch 38 completed, Last reward: -7.21 Average reward: -6.59\n",
            "Batch 39 completed, Last reward: -4.37 Average reward: -6.62\n",
            "Batch 40 completed, Last reward: -5.62 Average reward: -6.47\n",
            "Batch 41 completed, Last reward: -5.09 Average reward: -6.68\n",
            "Batch 42 completed, Last reward: -6.20 Average reward: -6.62\n",
            "Batch 43 completed, Last reward: -8.10 Average reward: -7.12\n",
            "Batch 44 completed, Last reward: -6.29 Average reward: -6.59\n",
            "Batch 45 completed, Last reward: -8.13 Average reward: -6.65\n",
            "Batch 46 completed, Last reward: -5.99 Average reward: -6.66\n",
            "Batch 47 completed, Last reward: -6.69 Average reward: -6.93\n",
            "Batch 48 completed, Last reward: -6.78 Average reward: -6.73\n",
            "Batch 49 completed, Last reward: -7.78 Average reward: -6.86\n",
            "Batch 50 completed, Last reward: -7.68 Average reward: -6.63\n",
            "Batch 51 completed, Last reward: -7.62 Average reward: -6.21\n",
            "Batch 52 completed, Last reward: -6.71 Average reward: -6.76\n",
            "Batch 53 completed, Last reward: -6.93 Average reward: -6.71\n",
            "Batch 54 completed, Last reward: -6.11 Average reward: -6.76\n",
            "Batch 55 completed, Last reward: -8.27 Average reward: -7.25\n",
            "Batch 56 completed, Last reward: -7.14 Average reward: -6.93\n",
            "Batch 57 completed, Last reward: -4.76 Average reward: -6.65\n",
            "Batch 58 completed, Last reward: -5.73 Average reward: -6.77\n",
            "Batch 59 completed, Last reward: -7.23 Average reward: -6.61\n",
            "Batch 60 completed, Last reward: -7.34 Average reward: -6.92\n",
            "Batch 61 completed, Last reward: -6.97 Average reward: -6.74\n",
            "Batch 62 completed, Last reward: -7.95 Average reward: -7.03\n",
            "Batch 63 completed, Last reward: -8.91 Average reward: -6.94\n",
            "Batch 64 completed, Last reward: -7.58 Average reward: -6.74\n",
            "Batch 65 completed, Last reward: -7.11 Average reward: -6.92\n",
            "Batch 66 completed, Last reward: -4.21 Average reward: -6.98\n",
            "Batch 67 completed, Last reward: -6.58 Average reward: -6.48\n",
            "Batch 68 completed, Last reward: -7.35 Average reward: -6.81\n",
            "Batch 69 completed, Last reward: -7.46 Average reward: -7.03\n",
            "Batch 70 completed, Last reward: -5.41 Average reward: -6.62\n",
            "Batch 71 completed, Last reward: -5.83 Average reward: -6.89\n",
            "Batch 72 completed, Last reward: -5.52 Average reward: -6.90\n",
            "Batch 73 completed, Last reward: -6.43 Average reward: -6.63\n",
            "Batch 74 completed, Last reward: -8.77 Average reward: -7.18\n",
            "Batch 75 completed, Last reward: -7.17 Average reward: -6.82\n",
            "Batch 76 completed, Last reward: -8.29 Average reward: -6.75\n",
            "Batch 77 completed, Last reward: -6.37 Average reward: -6.69\n",
            "Batch 78 completed, Last reward: -5.99 Average reward: -6.62\n",
            "Batch 79 completed, Last reward: -8.14 Average reward: -6.91\n",
            "Batch 80 completed, Last reward: -8.72 Average reward: -6.87\n",
            "Batch 81 completed, Last reward: -7.43 Average reward: -6.66\n",
            "Batch 82 completed, Last reward: -6.35 Average reward: -6.87\n",
            "Batch 83 completed, Last reward: -5.80 Average reward: -6.59\n",
            "Batch 84 completed, Last reward: -6.73 Average reward: -6.85\n",
            "Batch 85 completed, Last reward: -6.32 Average reward: -6.45\n"
          ]
        }
      ],
      "source": [
        "# We start the main loop over 500,000 timesteps\n",
        "env.reset()\n",
        "total_timesteps = 0\n",
        "obs = copy.deepcopy(env.reset())\n",
        "\n",
        "policy.train(int(max_timesteps), batch_size, tau)\n",
        "\n",
        "# Add the last policy evaluation to our list of evaluations and we save our model\n",
        "evaluations.append(evaluate_policy(policy))\n",
        "if save_models: policy.save(\"%s\" % (file_name), directory=\"./pytorch_models\")\n",
        "np.save(\"./results/%s\" % (file_name), evaluations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZsCwRpvBTPL",
        "outputId": "0ca3fc7d-f404-46a0-d8bc-f37c737abd85"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 0 completed, Last reward: -6.57 Average reward: -6.85\n",
            "Batch 1 completed, Last reward: -6.07 Average reward: -6.84\n",
            "Batch 2 completed, Last reward: -5.05 Average reward: -6.79\n",
            "Batch 3 completed, Last reward: -6.25 Average reward: -6.58\n",
            "Batch 4 completed, Last reward: -8.18 Average reward: -6.83\n",
            "Batch 5 completed, Last reward: -7.82 Average reward: -6.94\n",
            "Batch 6 completed, Last reward: -5.46 Average reward: -6.63\n",
            "Batch 7 completed, Last reward: -5.79 Average reward: -6.86\n",
            "Batch 8 completed, Last reward: -6.84 Average reward: -6.73\n",
            "Batch 9 completed, Last reward: -7.00 Average reward: -6.74\n",
            "Batch 10 completed, Last reward: -6.48 Average reward: -6.51\n",
            "Batch 11 completed, Last reward: -6.98 Average reward: -6.81\n",
            "Batch 12 completed, Last reward: -8.60 Average reward: -7.05\n",
            "Batch 13 completed, Last reward: -6.66 Average reward: -6.52\n",
            "Batch 14 completed, Last reward: -8.07 Average reward: -6.84\n",
            "Batch 15 completed, Last reward: -5.77 Average reward: -6.40\n",
            "Batch 16 completed, Last reward: -6.90 Average reward: -6.77\n",
            "Batch 17 completed, Last reward: -5.63 Average reward: -6.61\n",
            "Batch 18 completed, Last reward: -6.49 Average reward: -6.59\n",
            "Batch 19 completed, Last reward: -7.06 Average reward: -6.97\n",
            "Batch 20 completed, Last reward: -9.75 Average reward: -6.87\n",
            "Batch 21 completed, Last reward: -5.47 Average reward: -6.67\n",
            "Batch 22 completed, Last reward: -4.83 Average reward: -6.70\n",
            "Batch 23 completed, Last reward: -4.72 Average reward: -6.28\n",
            "Batch 24 completed, Last reward: -6.42 Average reward: -6.68\n",
            "Batch 25 completed, Last reward: -8.10 Average reward: -6.90\n",
            "Batch 26 completed, Last reward: -5.47 Average reward: -6.62\n",
            "Batch 27 completed, Last reward: -6.99 Average reward: -6.40\n",
            "Batch 28 completed, Last reward: -6.09 Average reward: -6.89\n",
            "Batch 29 completed, Last reward: -6.23 Average reward: -6.65\n",
            "Batch 30 completed, Last reward: -5.93 Average reward: -6.70\n",
            "Batch 31 completed, Last reward: -6.21 Average reward: -6.72\n",
            "Batch 32 completed, Last reward: -6.78 Average reward: -6.49\n",
            "Batch 33 completed, Last reward: -5.86 Average reward: -6.38\n",
            "Batch 34 completed, Last reward: -7.43 Average reward: -6.54\n",
            "Batch 35 completed, Last reward: -6.15 Average reward: -6.89\n",
            "Batch 36 completed, Last reward: -7.95 Average reward: -6.87\n",
            "Batch 37 completed, Last reward: -6.61 Average reward: -7.09\n",
            "Batch 38 completed, Last reward: -6.32 Average reward: -6.50\n",
            "Batch 39 completed, Last reward: -3.92 Average reward: -6.61\n",
            "Batch 40 completed, Last reward: -6.21 Average reward: -6.67\n",
            "Batch 41 completed, Last reward: -5.74 Average reward: -6.63\n",
            "Batch 42 completed, Last reward: -6.03 Average reward: -6.62\n",
            "Batch 43 completed, Last reward: -7.38 Average reward: -7.01\n",
            "Batch 44 completed, Last reward: -6.05 Average reward: -6.42\n",
            "Batch 45 completed, Last reward: -6.99 Average reward: -6.71\n",
            "Batch 46 completed, Last reward: -7.15 Average reward: -6.65\n",
            "Batch 47 completed, Last reward: -5.79 Average reward: -6.73\n",
            "Batch 48 completed, Last reward: -5.86 Average reward: -6.52\n",
            "Batch 49 completed, Last reward: -9.57 Average reward: -6.86\n",
            "Batch 50 completed, Last reward: -6.82 Average reward: -6.66\n",
            "Batch 51 completed, Last reward: -8.04 Average reward: -6.43\n",
            "Batch 52 completed, Last reward: -5.99 Average reward: -6.47\n",
            "Batch 53 completed, Last reward: -6.80 Average reward: -6.55\n",
            "Batch 54 completed, Last reward: -6.00 Average reward: -6.80\n",
            "Batch 55 completed, Last reward: -7.10 Average reward: -7.05\n",
            "Batch 56 completed, Last reward: -7.12 Average reward: -6.81\n",
            "Batch 57 completed, Last reward: -4.23 Average reward: -6.67\n",
            "Batch 58 completed, Last reward: -5.69 Average reward: -6.73\n",
            "Batch 59 completed, Last reward: -5.93 Average reward: -6.89\n",
            "Batch 60 completed, Last reward: -6.48 Average reward: -6.83\n",
            "Batch 61 completed, Last reward: -7.00 Average reward: -6.67\n",
            "Batch 62 completed, Last reward: -8.77 Average reward: -7.12\n",
            "Batch 63 completed, Last reward: -7.58 Average reward: -7.09\n",
            "Batch 64 completed, Last reward: -8.41 Average reward: -6.87\n",
            "Batch 65 completed, Last reward: -7.74 Average reward: -6.90\n",
            "Batch 66 completed, Last reward: -5.69 Average reward: -6.99\n",
            "Batch 67 completed, Last reward: -7.34 Average reward: -6.59\n",
            "Batch 68 completed, Last reward: -6.82 Average reward: -6.87\n",
            "Batch 69 completed, Last reward: -7.17 Average reward: -6.90\n",
            "Batch 70 completed, Last reward: -6.40 Average reward: -6.75\n",
            "Batch 71 completed, Last reward: -6.06 Average reward: -7.05\n",
            "Batch 72 completed, Last reward: -8.52 Average reward: -7.16\n",
            "Batch 73 completed, Last reward: -7.04 Average reward: -6.78\n",
            "Batch 74 completed, Last reward: -9.36 Average reward: -7.14\n",
            "Batch 75 completed, Last reward: -6.19 Average reward: -6.88\n",
            "Batch 76 completed, Last reward: -9.08 Average reward: -6.87\n",
            "Batch 77 completed, Last reward: -4.95 Average reward: -6.50\n",
            "Batch 78 completed, Last reward: -5.05 Average reward: -6.68\n",
            "Batch 79 completed, Last reward: -7.05 Average reward: -6.79\n",
            "Batch 80 completed, Last reward: -9.10 Average reward: -6.72\n",
            "Batch 81 completed, Last reward: -6.77 Average reward: -6.96\n",
            "Batch 82 completed, Last reward: -7.18 Average reward: -6.69\n",
            "Batch 83 completed, Last reward: -6.35 Average reward: -6.57\n",
            "Batch 84 completed, Last reward: -6.99 Average reward: -6.86\n",
            "Batch 85 completed, Last reward: -7.09 Average reward: -6.85\n",
            "Batch 86 completed, Last reward: -7.56 Average reward: -6.81\n",
            "Batch 87 completed, Last reward: -7.96 Average reward: -7.14\n",
            "Batch 88 completed, Last reward: -8.19 Average reward: -6.64\n",
            "Batch 89 completed, Last reward: -8.73 Average reward: -6.58\n",
            "Batch 90 completed, Last reward: -7.50 Average reward: -6.91\n",
            "Batch 91 completed, Last reward: -6.23 Average reward: -6.74\n",
            "Batch 92 completed, Last reward: -7.58 Average reward: -6.80\n",
            "Batch 93 completed, Last reward: -9.24 Average reward: -7.08\n",
            "Batch 94 completed, Last reward: -6.74 Average reward: -6.86\n",
            "Batch 95 completed, Last reward: -8.83 Average reward: -6.75\n",
            "Batch 96 completed, Last reward: -8.19 Average reward: -6.90\n",
            "Batch 97 completed, Last reward: -5.68 Average reward: -6.68\n",
            "Batch 98 completed, Last reward: -7.78 Average reward: -6.97\n",
            "Batch 99 completed, Last reward: -8.22 Average reward: -7.20\n",
            "Batch 100 completed, Last reward: -7.62 Average reward: -6.89\n",
            "Batch 101 completed, Last reward: -7.73 Average reward: -6.85\n",
            "Batch 102 completed, Last reward: -6.38 Average reward: -7.11\n",
            "Batch 103 completed, Last reward: -6.06 Average reward: -6.70\n",
            "Batch 104 completed, Last reward: -5.75 Average reward: -6.44\n",
            "Batch 105 completed, Last reward: -4.62 Average reward: -6.92\n",
            "Batch 106 completed, Last reward: -6.22 Average reward: -6.96\n",
            "Batch 107 completed, Last reward: -7.08 Average reward: -6.96\n",
            "Batch 108 completed, Last reward: -7.68 Average reward: -6.58\n",
            "Batch 109 completed, Last reward: -6.56 Average reward: -7.12\n",
            "Batch 110 completed, Last reward: -4.86 Average reward: -6.56\n",
            "Batch 111 completed, Last reward: -7.07 Average reward: -6.99\n",
            "Batch 112 completed, Last reward: -8.00 Average reward: -6.97\n",
            "Batch 113 completed, Last reward: -7.66 Average reward: -6.96\n",
            "Batch 114 completed, Last reward: -6.89 Average reward: -6.67\n",
            "Batch 115 completed, Last reward: -5.07 Average reward: -6.82\n",
            "Batch 116 completed, Last reward: -6.58 Average reward: -6.61\n",
            "Batch 117 completed, Last reward: -7.80 Average reward: -7.03\n",
            "Batch 118 completed, Last reward: -5.16 Average reward: -6.76\n",
            "Batch 119 completed, Last reward: -6.45 Average reward: -6.73\n",
            "Batch 120 completed, Last reward: -5.58 Average reward: -7.03\n",
            "Batch 121 completed, Last reward: -7.76 Average reward: -6.84\n",
            "Batch 122 completed, Last reward: -8.69 Average reward: -7.16\n",
            "Batch 123 completed, Last reward: -6.90 Average reward: -6.92\n",
            "Batch 124 completed, Last reward: -5.72 Average reward: -6.66\n",
            "Batch 125 completed, Last reward: -6.96 Average reward: -6.84\n",
            "Batch 126 completed, Last reward: -5.88 Average reward: -6.60\n",
            "Batch 127 completed, Last reward: -8.52 Average reward: -6.78\n",
            "Batch 128 completed, Last reward: -5.51 Average reward: -7.09\n",
            "Batch 129 completed, Last reward: -6.51 Average reward: -6.43\n",
            "Batch 130 completed, Last reward: -7.45 Average reward: -6.54\n",
            "Batch 131 completed, Last reward: -7.42 Average reward: -6.80\n",
            "Batch 132 completed, Last reward: -7.46 Average reward: -6.74\n",
            "Batch 133 completed, Last reward: -7.02 Average reward: -6.77\n",
            "Batch 134 completed, Last reward: -7.00 Average reward: -6.85\n",
            "Batch 135 completed, Last reward: -5.97 Average reward: -6.87\n",
            "Batch 136 completed, Last reward: -5.58 Average reward: -6.50\n",
            "Batch 137 completed, Last reward: -8.98 Average reward: -6.97\n",
            "Batch 138 completed, Last reward: -8.45 Average reward: -7.06\n",
            "Batch 139 completed, Last reward: -7.22 Average reward: -6.60\n",
            "Batch 140 completed, Last reward: -6.71 Average reward: -6.67\n",
            "Batch 141 completed, Last reward: -4.60 Average reward: -6.67\n",
            "Batch 142 completed, Last reward: -8.05 Average reward: -6.48\n",
            "Batch 143 completed, Last reward: -6.80 Average reward: -6.74\n",
            "Batch 144 completed, Last reward: -4.89 Average reward: -6.49\n",
            "Batch 145 completed, Last reward: -4.94 Average reward: -6.47\n",
            "Batch 146 completed, Last reward: -6.96 Average reward: -6.91\n",
            "Batch 147 completed, Last reward: -6.98 Average reward: -6.64\n",
            "Batch 148 completed, Last reward: -7.58 Average reward: -7.02\n",
            "Batch 149 completed, Last reward: -6.01 Average reward: -6.77\n",
            "Batch 150 completed, Last reward: -4.87 Average reward: -6.73\n",
            "Batch 151 completed, Last reward: -6.08 Average reward: -6.63\n",
            "Batch 152 completed, Last reward: -5.80 Average reward: -6.74\n",
            "Batch 153 completed, Last reward: -7.09 Average reward: -6.68\n",
            "Batch 154 completed, Last reward: -6.34 Average reward: -6.97\n",
            "Batch 155 completed, Last reward: -5.58 Average reward: -6.68\n",
            "Batch 156 completed, Last reward: -8.38 Average reward: -6.81\n",
            "Batch 157 completed, Last reward: -7.79 Average reward: -6.60\n",
            "Batch 158 completed, Last reward: -5.53 Average reward: -6.75\n",
            "Batch 159 completed, Last reward: -8.70 Average reward: -6.65\n",
            "Batch 160 completed, Last reward: -6.47 Average reward: -6.59\n",
            "Batch 161 completed, Last reward: -6.05 Average reward: -6.80\n",
            "Batch 162 completed, Last reward: -7.25 Average reward: -6.72\n",
            "Batch 163 completed, Last reward: -9.63 Average reward: -6.97\n",
            "Batch 164 completed, Last reward: -7.01 Average reward: -6.66\n",
            "Batch 165 completed, Last reward: -7.42 Average reward: -6.81\n"
          ]
        }
      ],
      "source": [
        "# We start the main loop over 500,000 timesteps\n",
        "env.reset()\n",
        "total_timesteps = 0\n",
        "obs = copy.deepcopy(env.reset())\n",
        "\n",
        "policy.train(int(max_timesteps), batch_size, tau)\n",
        "\n",
        "# Add the last policy evaluation to our list of evaluations and we save our model\n",
        "evaluations.append(evaluate_policy(policy))\n",
        "if save_models: policy.save(\"%s\" % (file_name), directory=\"./pytorch_models\")\n",
        "np.save(\"./results/%s\" % (file_name), evaluations)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We start the main loop over 500,000 timesteps\n",
        "env.reset()\n",
        "total_timesteps = 0\n",
        "obs = copy.deepcopy(env.reset())\n",
        "\n",
        "policy.train(int(max_timesteps), batch_size, tau)\n",
        "\n",
        "# Add the last policy evaluation to our list of evaluations and we save our model\n",
        "evaluations.append(evaluate_policy(policy))\n",
        "if save_models: policy.save(\"%s\" % (file_name), directory=\"./pytorch_models\")\n",
        "np.save(\"./results/%s\" % (file_name), evaluations)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UzG14FLCR9Dh",
        "outputId": "76d9547e-2877-4e00-d98e-6a2c1f333339"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 0 completed, Last reward: -3.88 Average reward: -6.01\n",
            "Batch 1 completed, Last reward: -7.89 Average reward: -6.17\n",
            "Batch 2 completed, Last reward: -5.75 Average reward: -5.91\n",
            "Batch 3 completed, Last reward: -10.32 Average reward: -6.36\n",
            "Batch 4 completed, Last reward: -8.01 Average reward: -6.16\n",
            "Batch 5 completed, Last reward: -8.32 Average reward: -6.58\n",
            "Batch 6 completed, Last reward: -6.17 Average reward: -6.55\n",
            "Batch 7 completed, Last reward: -5.70 Average reward: -6.02\n",
            "Batch 8 completed, Last reward: -6.59 Average reward: -6.28\n",
            "Batch 9 completed, Last reward: -6.72 Average reward: -6.13\n",
            "Batch 10 completed, Last reward: -7.43 Average reward: -6.16\n",
            "Batch 11 completed, Last reward: -5.95 Average reward: -5.92\n",
            "Batch 12 completed, Last reward: -6.84 Average reward: -6.43\n",
            "Batch 13 completed, Last reward: -4.40 Average reward: -6.08\n",
            "Batch 14 completed, Last reward: -5.64 Average reward: -6.23\n",
            "Batch 15 completed, Last reward: -6.17 Average reward: -6.50\n",
            "Batch 16 completed, Last reward: -6.58 Average reward: -6.12\n",
            "Batch 17 completed, Last reward: -6.14 Average reward: -6.38\n",
            "Batch 18 completed, Last reward: -7.02 Average reward: -6.41\n",
            "Batch 19 completed, Last reward: -8.18 Average reward: -6.05\n",
            "Batch 20 completed, Last reward: -6.49 Average reward: -6.30\n",
            "Batch 21 completed, Last reward: -6.69 Average reward: -6.18\n",
            "Batch 22 completed, Last reward: -8.90 Average reward: -6.19\n",
            "Batch 23 completed, Last reward: -7.09 Average reward: -6.23\n",
            "Batch 24 completed, Last reward: -6.16 Average reward: -6.14\n",
            "Batch 25 completed, Last reward: -6.57 Average reward: -6.29\n",
            "Batch 26 completed, Last reward: -7.40 Average reward: -6.16\n",
            "Batch 27 completed, Last reward: -7.92 Average reward: -6.19\n",
            "Batch 28 completed, Last reward: -8.26 Average reward: -6.30\n",
            "Batch 29 completed, Last reward: -6.13 Average reward: -6.14\n",
            "Batch 30 completed, Last reward: -7.72 Average reward: -6.42\n",
            "Batch 31 completed, Last reward: -6.00 Average reward: -5.90\n",
            "Batch 32 completed, Last reward: -7.17 Average reward: -6.37\n",
            "Batch 33 completed, Last reward: -8.29 Average reward: -6.23\n",
            "Batch 34 completed, Last reward: -8.61 Average reward: -6.25\n",
            "Batch 35 completed, Last reward: -8.76 Average reward: -6.27\n",
            "Batch 36 completed, Last reward: -7.23 Average reward: -6.02\n",
            "Batch 37 completed, Last reward: -7.78 Average reward: -6.50\n",
            "Batch 38 completed, Last reward: -7.63 Average reward: -6.30\n",
            "Batch 39 completed, Last reward: -6.62 Average reward: -5.89\n",
            "Batch 40 completed, Last reward: -5.86 Average reward: -6.42\n",
            "Batch 41 completed, Last reward: -8.08 Average reward: -6.51\n",
            "Batch 42 completed, Last reward: -7.50 Average reward: -6.13\n",
            "Batch 43 completed, Last reward: -7.49 Average reward: -6.17\n",
            "Batch 44 completed, Last reward: -6.11 Average reward: -6.31\n",
            "Batch 45 completed, Last reward: -6.09 Average reward: -6.34\n",
            "Batch 46 completed, Last reward: -6.77 Average reward: -6.26\n",
            "Batch 47 completed, Last reward: -6.58 Average reward: -6.15\n",
            "Batch 48 completed, Last reward: -7.87 Average reward: -6.22\n",
            "Batch 49 completed, Last reward: -7.12 Average reward: -6.16\n",
            "Batch 50 completed, Last reward: -5.16 Average reward: -6.47\n",
            "Batch 51 completed, Last reward: -6.01 Average reward: -6.38\n",
            "Batch 52 completed, Last reward: -5.49 Average reward: -6.15\n",
            "Batch 53 completed, Last reward: -8.80 Average reward: -6.23\n",
            "Batch 54 completed, Last reward: -5.73 Average reward: -6.49\n",
            "Batch 55 completed, Last reward: -8.44 Average reward: -5.95\n",
            "Batch 56 completed, Last reward: -8.71 Average reward: -5.94\n",
            "Batch 57 completed, Last reward: -6.32 Average reward: -6.05\n",
            "Batch 58 completed, Last reward: -6.29 Average reward: -6.40\n",
            "Batch 59 completed, Last reward: -6.67 Average reward: -6.22\n",
            "Batch 60 completed, Last reward: -7.94 Average reward: -6.45\n",
            "Batch 61 completed, Last reward: -6.11 Average reward: -6.40\n",
            "Batch 62 completed, Last reward: -5.11 Average reward: -5.97\n",
            "Batch 63 completed, Last reward: -6.08 Average reward: -6.34\n",
            "Batch 64 completed, Last reward: -6.80 Average reward: -6.33\n",
            "Batch 65 completed, Last reward: -6.26 Average reward: -6.05\n",
            "Batch 66 completed, Last reward: -5.62 Average reward: -6.08\n",
            "Batch 67 completed, Last reward: -7.32 Average reward: -6.09\n",
            "Batch 68 completed, Last reward: -7.07 Average reward: -6.32\n",
            "Batch 69 completed, Last reward: -5.72 Average reward: -5.91\n",
            "Batch 70 completed, Last reward: -5.56 Average reward: -6.09\n",
            "Batch 71 completed, Last reward: -6.44 Average reward: -6.24\n",
            "Batch 72 completed, Last reward: -9.04 Average reward: -6.50\n",
            "Batch 73 completed, Last reward: -9.00 Average reward: -6.32\n",
            "Batch 74 completed, Last reward: -6.50 Average reward: -6.38\n",
            "Batch 75 completed, Last reward: -6.33 Average reward: -6.26\n",
            "Batch 76 completed, Last reward: -7.26 Average reward: -6.24\n",
            "Batch 77 completed, Last reward: -6.93 Average reward: -6.45\n",
            "Batch 78 completed, Last reward: -5.78 Average reward: -6.21\n",
            "Batch 79 completed, Last reward: -5.38 Average reward: -6.01\n",
            "Batch 80 completed, Last reward: -5.98 Average reward: -6.37\n",
            "Batch 81 completed, Last reward: -6.00 Average reward: -6.41\n",
            "Batch 82 completed, Last reward: -6.59 Average reward: -6.35\n",
            "Batch 83 completed, Last reward: -8.55 Average reward: -6.28\n",
            "Batch 84 completed, Last reward: -8.08 Average reward: -6.23\n",
            "Batch 85 completed, Last reward: -6.75 Average reward: -6.16\n",
            "Batch 86 completed, Last reward: -6.17 Average reward: -6.34\n",
            "Batch 87 completed, Last reward: -6.80 Average reward: -6.28\n",
            "Batch 88 completed, Last reward: -7.55 Average reward: -6.30\n",
            "Batch 89 completed, Last reward: -8.73 Average reward: -6.29\n",
            "Batch 90 completed, Last reward: -6.45 Average reward: -6.32\n",
            "Batch 91 completed, Last reward: -7.55 Average reward: -6.00\n",
            "Batch 92 completed, Last reward: -8.91 Average reward: -6.33\n",
            "Batch 93 completed, Last reward: -7.88 Average reward: -5.91\n",
            "Batch 94 completed, Last reward: -7.41 Average reward: -6.43\n",
            "Batch 95 completed, Last reward: -7.06 Average reward: -6.20\n",
            "Batch 96 completed, Last reward: -5.08 Average reward: -6.21\n",
            "Batch 97 completed, Last reward: -7.30 Average reward: -6.26\n",
            "Batch 98 completed, Last reward: -6.14 Average reward: -6.31\n",
            "Batch 99 completed, Last reward: -3.48 Average reward: -5.90\n",
            "Batch 100 completed, Last reward: -7.05 Average reward: -6.39\n",
            "Batch 101 completed, Last reward: -7.02 Average reward: -6.20\n",
            "Batch 102 completed, Last reward: -7.20 Average reward: -6.50\n",
            "Batch 103 completed, Last reward: -6.20 Average reward: -6.04\n",
            "Batch 104 completed, Last reward: -6.09 Average reward: -6.16\n",
            "Batch 105 completed, Last reward: -7.32 Average reward: -6.04\n",
            "Batch 106 completed, Last reward: -7.64 Average reward: -6.50\n",
            "Batch 107 completed, Last reward: -7.29 Average reward: -6.36\n",
            "Batch 108 completed, Last reward: -6.65 Average reward: -6.10\n",
            "Batch 109 completed, Last reward: -7.99 Average reward: -6.53\n",
            "Batch 110 completed, Last reward: -6.03 Average reward: -6.08\n",
            "Batch 111 completed, Last reward: -6.62 Average reward: -6.32\n",
            "Batch 112 completed, Last reward: -8.45 Average reward: -6.57\n",
            "Batch 113 completed, Last reward: -6.72 Average reward: -6.42\n",
            "Batch 114 completed, Last reward: -5.31 Average reward: -5.89\n",
            "Batch 115 completed, Last reward: -6.54 Average reward: -6.10\n",
            "Batch 116 completed, Last reward: -8.08 Average reward: -6.15\n",
            "Batch 117 completed, Last reward: -6.17 Average reward: -6.33\n",
            "Batch 118 completed, Last reward: -8.62 Average reward: -6.17\n",
            "Batch 119 completed, Last reward: -10.13 Average reward: -6.60\n",
            "Batch 120 completed, Last reward: -6.18 Average reward: -6.31\n",
            "Batch 121 completed, Last reward: -7.16 Average reward: -6.57\n",
            "Batch 122 completed, Last reward: -8.52 Average reward: -5.99\n",
            "Batch 123 completed, Last reward: -7.42 Average reward: -6.03\n",
            "Batch 124 completed, Last reward: -6.74 Average reward: -6.45\n",
            "Batch 125 completed, Last reward: -6.57 Average reward: -6.41\n",
            "Batch 126 completed, Last reward: -7.77 Average reward: -6.28\n",
            "Batch 127 completed, Last reward: -7.28 Average reward: -6.26\n",
            "Batch 128 completed, Last reward: -8.48 Average reward: -6.28\n",
            "Batch 129 completed, Last reward: -6.31 Average reward: -6.21\n",
            "Batch 130 completed, Last reward: -6.32 Average reward: -6.20\n",
            "Batch 131 completed, Last reward: -8.14 Average reward: -6.59\n",
            "Batch 132 completed, Last reward: -7.72 Average reward: -6.22\n",
            "Batch 133 completed, Last reward: -8.53 Average reward: -6.44\n",
            "Batch 134 completed, Last reward: -7.74 Average reward: -6.24\n",
            "Batch 135 completed, Last reward: -6.97 Average reward: -6.02\n",
            "Batch 136 completed, Last reward: -8.14 Average reward: -6.26\n",
            "Batch 137 completed, Last reward: -7.36 Average reward: -6.32\n",
            "Batch 138 completed, Last reward: -7.44 Average reward: -6.27\n",
            "Batch 139 completed, Last reward: -6.21 Average reward: -6.61\n",
            "Batch 140 completed, Last reward: -5.37 Average reward: -6.06\n",
            "Batch 141 completed, Last reward: -6.71 Average reward: -5.98\n",
            "Batch 142 completed, Last reward: -7.14 Average reward: -6.19\n",
            "Batch 143 completed, Last reward: -6.55 Average reward: -6.38\n",
            "Batch 144 completed, Last reward: -4.35 Average reward: -6.37\n",
            "Batch 145 completed, Last reward: -7.81 Average reward: -6.32\n",
            "Batch 146 completed, Last reward: -5.69 Average reward: -6.25\n",
            "Batch 147 completed, Last reward: -4.95 Average reward: -6.09\n",
            "Batch 148 completed, Last reward: -6.01 Average reward: -6.08\n",
            "Batch 149 completed, Last reward: -6.66 Average reward: -6.40\n",
            "Batch 150 completed, Last reward: -5.97 Average reward: -5.75\n",
            "Batch 151 completed, Last reward: -6.55 Average reward: -6.34\n",
            "Batch 152 completed, Last reward: -5.84 Average reward: -6.14\n",
            "Batch 153 completed, Last reward: -4.35 Average reward: -5.83\n",
            "Batch 154 completed, Last reward: -7.31 Average reward: -6.33\n",
            "Batch 155 completed, Last reward: -8.22 Average reward: -6.19\n",
            "Batch 156 completed, Last reward: -8.48 Average reward: -6.69\n",
            "Batch 157 completed, Last reward: -5.65 Average reward: -5.99\n",
            "Batch 158 completed, Last reward: -5.62 Average reward: -6.17\n",
            "Batch 159 completed, Last reward: -6.93 Average reward: -6.56\n",
            "Batch 160 completed, Last reward: -4.84 Average reward: -6.07\n",
            "Batch 161 completed, Last reward: -8.53 Average reward: -6.08\n",
            "Batch 162 completed, Last reward: -6.84 Average reward: -6.27\n",
            "Batch 163 completed, Last reward: -5.92 Average reward: -6.17\n",
            "Batch 164 completed, Last reward: -7.46 Average reward: -6.32\n",
            "Batch 165 completed, Last reward: -7.11 Average reward: -6.54\n",
            "Batch 166 completed, Last reward: -6.41 Average reward: -6.36\n",
            "Batch 167 completed, Last reward: -6.41 Average reward: -6.01\n",
            "Batch 168 completed, Last reward: -8.12 Average reward: -6.45\n",
            "Batch 169 completed, Last reward: -7.63 Average reward: -6.28\n",
            "Batch 170 completed, Last reward: -6.82 Average reward: -6.28\n",
            "Batch 171 completed, Last reward: -6.87 Average reward: -6.36\n",
            "Batch 172 completed, Last reward: -7.54 Average reward: -6.16\n",
            "Batch 173 completed, Last reward: -6.52 Average reward: -6.15\n",
            "Batch 174 completed, Last reward: -6.10 Average reward: -5.89\n",
            "Batch 175 completed, Last reward: -9.99 Average reward: -6.56\n",
            "Batch 176 completed, Last reward: -8.14 Average reward: -6.30\n",
            "Batch 177 completed, Last reward: -7.24 Average reward: -5.95\n",
            "Batch 178 completed, Last reward: -7.75 Average reward: -6.13\n",
            "Batch 179 completed, Last reward: -4.27 Average reward: -5.80\n",
            "Batch 180 completed, Last reward: -7.36 Average reward: -6.49\n",
            "Batch 181 completed, Last reward: -7.12 Average reward: -6.28\n",
            "Batch 182 completed, Last reward: -4.79 Average reward: -6.05\n",
            "Batch 183 completed, Last reward: -6.52 Average reward: -6.20\n",
            "Batch 184 completed, Last reward: -8.79 Average reward: -6.62\n",
            "Batch 185 completed, Last reward: -6.00 Average reward: -6.15\n",
            "Batch 186 completed, Last reward: -7.42 Average reward: -6.34\n",
            "Batch 187 completed, Last reward: -7.22 Average reward: -6.32\n",
            "Batch 188 completed, Last reward: -4.56 Average reward: -6.10\n",
            "Batch 189 completed, Last reward: -5.02 Average reward: -6.39\n",
            "Batch 190 completed, Last reward: -8.24 Average reward: -6.42\n",
            "Batch 191 completed, Last reward: -5.97 Average reward: -5.93\n",
            "Batch 192 completed, Last reward: -6.68 Average reward: -6.16\n",
            "Batch 193 completed, Last reward: -7.92 Average reward: -6.52\n",
            "Batch 194 completed, Last reward: -8.43 Average reward: -6.28\n",
            "Batch 195 completed, Last reward: -5.68 Average reward: -5.97\n",
            "Batch 196 completed, Last reward: -5.31 Average reward: -6.20\n",
            "Batch 197 completed, Last reward: -7.08 Average reward: -6.22\n",
            "Batch 198 completed, Last reward: -6.65 Average reward: -6.55\n",
            "Batch 199 completed, Last reward: -6.92 Average reward: -6.31\n",
            "Batch 200 completed, Last reward: -5.12 Average reward: -6.16\n",
            "Batch 201 completed, Last reward: -7.92 Average reward: -6.39\n",
            "Batch 202 completed, Last reward: -6.01 Average reward: -5.99\n",
            "Batch 203 completed, Last reward: -7.33 Average reward: -6.21\n",
            "Batch 204 completed, Last reward: -5.94 Average reward: -5.72\n",
            "Batch 205 completed, Last reward: -7.18 Average reward: -6.41\n",
            "Batch 206 completed, Last reward: -5.32 Average reward: -5.89\n",
            "Batch 207 completed, Last reward: -7.65 Average reward: -6.31\n",
            "Batch 208 completed, Last reward: -5.11 Average reward: -6.30\n",
            "Batch 209 completed, Last reward: -7.05 Average reward: -6.24\n",
            "Batch 210 completed, Last reward: -6.61 Average reward: -6.11\n",
            "Batch 211 completed, Last reward: -8.05 Average reward: -6.36\n",
            "Batch 212 completed, Last reward: -7.19 Average reward: -6.09\n",
            "Batch 213 completed, Last reward: -7.89 Average reward: -6.47\n",
            "Batch 214 completed, Last reward: -6.99 Average reward: -6.38\n",
            "Batch 215 completed, Last reward: -4.91 Average reward: -6.35\n",
            "Batch 216 completed, Last reward: -6.17 Average reward: -6.15\n",
            "Batch 217 completed, Last reward: -6.28 Average reward: -6.08\n",
            "Batch 218 completed, Last reward: -6.04 Average reward: -6.32\n",
            "Batch 219 completed, Last reward: -8.31 Average reward: -6.54\n",
            "Batch 220 completed, Last reward: -7.53 Average reward: -6.14\n",
            "Batch 221 completed, Last reward: -6.98 Average reward: -6.23\n",
            "Batch 222 completed, Last reward: -8.30 Average reward: -6.64\n",
            "Batch 223 completed, Last reward: -6.93 Average reward: -6.19\n",
            "Batch 224 completed, Last reward: -5.51 Average reward: -5.95\n",
            "Batch 225 completed, Last reward: -7.48 Average reward: -6.40\n",
            "Batch 226 completed, Last reward: -5.49 Average reward: -5.89\n",
            "Batch 227 completed, Last reward: -7.87 Average reward: -6.42\n",
            "Batch 228 completed, Last reward: -7.23 Average reward: -6.35\n",
            "Batch 229 completed, Last reward: -4.71 Average reward: -5.97\n",
            "Batch 230 completed, Last reward: -7.86 Average reward: -6.38\n",
            "Batch 231 completed, Last reward: -5.75 Average reward: -6.32\n",
            "Batch 232 completed, Last reward: -6.56 Average reward: -6.16\n",
            "Batch 233 completed, Last reward: -5.44 Average reward: -6.00\n",
            "Batch 234 completed, Last reward: -5.79 Average reward: -5.96\n",
            "Batch 235 completed, Last reward: -7.02 Average reward: -6.39\n",
            "Batch 236 completed, Last reward: -6.82 Average reward: -6.48\n",
            "Batch 237 completed, Last reward: -8.85 Average reward: -6.36\n",
            "Batch 238 completed, Last reward: -6.98 Average reward: -6.40\n",
            "Batch 239 completed, Last reward: -7.39 Average reward: -6.28\n",
            "Batch 240 completed, Last reward: -5.15 Average reward: -6.10\n",
            "Batch 241 completed, Last reward: -6.68 Average reward: -6.33\n",
            "Batch 242 completed, Last reward: -8.06 Average reward: -6.14\n",
            "Batch 243 completed, Last reward: -6.74 Average reward: -6.45\n",
            "Batch 244 completed, Last reward: -6.55 Average reward: -6.31\n",
            "Batch 245 completed, Last reward: -7.55 Average reward: -6.55\n",
            "Batch 246 completed, Last reward: -7.94 Average reward: -6.18\n",
            "Batch 247 completed, Last reward: -4.95 Average reward: -6.50\n",
            "Batch 248 completed, Last reward: -5.58 Average reward: -6.26\n",
            "Batch 249 completed, Last reward: -9.15 Average reward: -6.13\n",
            "Batch 250 completed, Last reward: -7.08 Average reward: -6.32\n",
            "Batch 251 completed, Last reward: -6.42 Average reward: -6.33\n",
            "Batch 252 completed, Last reward: -6.72 Average reward: -6.42\n",
            "Batch 253 completed, Last reward: -6.02 Average reward: -6.11\n",
            "Batch 254 completed, Last reward: -6.32 Average reward: -6.35\n",
            "Batch 255 completed, Last reward: -7.80 Average reward: -6.10\n",
            "Batch 256 completed, Last reward: -6.18 Average reward: -5.99\n",
            "Batch 257 completed, Last reward: -9.40 Average reward: -6.28\n",
            "Batch 258 completed, Last reward: -6.91 Average reward: -5.79\n",
            "Batch 259 completed, Last reward: -6.18 Average reward: -5.85\n",
            "Batch 260 completed, Last reward: -4.95 Average reward: -6.19\n",
            "Batch 261 completed, Last reward: -6.06 Average reward: -6.10\n",
            "Batch 262 completed, Last reward: -7.26 Average reward: -6.17\n",
            "Batch 263 completed, Last reward: -6.82 Average reward: -6.11\n",
            "Batch 264 completed, Last reward: -8.54 Average reward: -6.23\n",
            "Batch 265 completed, Last reward: -7.38 Average reward: -6.21\n",
            "Batch 266 completed, Last reward: -5.66 Average reward: -6.06\n",
            "Batch 267 completed, Last reward: -8.84 Average reward: -6.35\n",
            "Batch 268 completed, Last reward: -8.01 Average reward: -6.38\n",
            "Batch 269 completed, Last reward: -6.17 Average reward: -6.23\n",
            "Batch 270 completed, Last reward: -7.55 Average reward: -6.25\n",
            "Batch 271 completed, Last reward: -7.56 Average reward: -6.30\n",
            "Batch 272 completed, Last reward: -6.70 Average reward: -6.34\n",
            "Batch 273 completed, Last reward: -6.07 Average reward: -6.16\n",
            "Batch 274 completed, Last reward: -7.73 Average reward: -6.47\n",
            "Batch 275 completed, Last reward: -5.68 Average reward: -6.17\n",
            "Batch 276 completed, Last reward: -5.96 Average reward: -6.08\n",
            "Batch 277 completed, Last reward: -8.02 Average reward: -6.50\n",
            "Batch 278 completed, Last reward: -8.82 Average reward: -5.95\n",
            "Batch 279 completed, Last reward: -7.24 Average reward: -6.20\n",
            "Batch 280 completed, Last reward: -8.50 Average reward: -6.48\n",
            "Batch 281 completed, Last reward: -5.23 Average reward: -6.28\n",
            "Batch 282 completed, Last reward: -5.97 Average reward: -6.57\n",
            "Batch 283 completed, Last reward: -7.54 Average reward: -6.17\n",
            "Batch 284 completed, Last reward: -8.81 Average reward: -6.26\n",
            "Batch 285 completed, Last reward: -4.37 Average reward: -5.89\n",
            "Batch 286 completed, Last reward: -6.26 Average reward: -6.17\n",
            "Batch 287 completed, Last reward: -8.11 Average reward: -6.08\n",
            "Batch 288 completed, Last reward: -5.33 Average reward: -6.40\n",
            "Batch 289 completed, Last reward: -5.17 Average reward: -6.23\n",
            "Batch 290 completed, Last reward: -7.30 Average reward: -6.43\n",
            "Batch 291 completed, Last reward: -5.33 Average reward: -6.14\n",
            "Batch 292 completed, Last reward: -7.35 Average reward: -6.11\n",
            "Batch 293 completed, Last reward: -7.28 Average reward: -6.36\n",
            "Batch 294 completed, Last reward: -8.10 Average reward: -6.46\n",
            "Batch 295 completed, Last reward: -7.29 Average reward: -6.26\n",
            "Batch 296 completed, Last reward: -8.04 Average reward: -6.10\n",
            "Batch 297 completed, Last reward: -7.87 Average reward: -6.26\n",
            "Batch 298 completed, Last reward: -7.28 Average reward: -6.05\n",
            "Batch 299 completed, Last reward: -6.03 Average reward: -6.30\n",
            "Batch 300 completed, Last reward: -5.38 Average reward: -6.50\n",
            "Batch 301 completed, Last reward: -5.29 Average reward: -6.34\n",
            "Batch 302 completed, Last reward: -7.39 Average reward: -6.21\n",
            "Batch 303 completed, Last reward: -7.15 Average reward: -5.79\n",
            "Batch 304 completed, Last reward: -7.95 Average reward: -6.59\n",
            "Batch 305 completed, Last reward: -7.03 Average reward: -6.25\n",
            "Batch 306 completed, Last reward: -8.11 Average reward: -6.30\n",
            "Batch 307 completed, Last reward: -7.30 Average reward: -6.09\n",
            "Batch 308 completed, Last reward: -6.06 Average reward: -6.25\n",
            "Batch 309 completed, Last reward: -7.68 Average reward: -6.53\n",
            "Batch 310 completed, Last reward: -5.88 Average reward: -6.13\n",
            "Batch 311 completed, Last reward: -7.17 Average reward: -6.30\n",
            "Batch 312 completed, Last reward: -6.91 Average reward: -6.12\n",
            "Batch 313 completed, Last reward: -6.86 Average reward: -6.35\n",
            "Batch 314 completed, Last reward: -7.39 Average reward: -6.48\n",
            "Batch 315 completed, Last reward: -6.73 Average reward: -5.90\n",
            "Batch 316 completed, Last reward: -5.64 Average reward: -5.87\n",
            "Batch 317 completed, Last reward: -6.67 Average reward: -6.33\n",
            "Batch 318 completed, Last reward: -5.67 Average reward: -6.17\n",
            "Batch 319 completed, Last reward: -4.04 Average reward: -6.05\n",
            "Batch 320 completed, Last reward: -5.00 Average reward: -6.39\n",
            "Batch 321 completed, Last reward: -5.79 Average reward: -6.17\n",
            "Batch 322 completed, Last reward: -5.72 Average reward: -6.21\n",
            "Batch 323 completed, Last reward: -7.28 Average reward: -6.20\n",
            "Batch 324 completed, Last reward: -6.47 Average reward: -6.24\n",
            "Batch 325 completed, Last reward: -6.74 Average reward: -6.16\n",
            "Batch 326 completed, Last reward: -5.92 Average reward: -6.03\n",
            "Batch 327 completed, Last reward: -6.11 Average reward: -6.25\n",
            "Batch 328 completed, Last reward: -7.09 Average reward: -6.33\n",
            "Batch 329 completed, Last reward: -6.38 Average reward: -6.14\n",
            "Batch 330 completed, Last reward: -8.67 Average reward: -6.45\n",
            "Batch 331 completed, Last reward: -7.33 Average reward: -6.26\n",
            "Batch 332 completed, Last reward: -6.31 Average reward: -6.03\n",
            "Batch 333 completed, Last reward: -5.96 Average reward: -6.20\n",
            "Batch 334 completed, Last reward: -9.69 Average reward: -6.24\n",
            "Batch 335 completed, Last reward: -6.64 Average reward: -6.34\n",
            "Batch 336 completed, Last reward: -7.40 Average reward: -6.15\n",
            "Batch 337 completed, Last reward: -6.97 Average reward: -6.07\n",
            "Batch 338 completed, Last reward: -6.05 Average reward: -6.12\n",
            "Batch 339 completed, Last reward: -6.71 Average reward: -6.34\n",
            "Batch 340 completed, Last reward: -6.06 Average reward: -6.11\n",
            "Batch 341 completed, Last reward: -3.86 Average reward: -6.12\n",
            "Batch 342 completed, Last reward: -6.55 Average reward: -6.45\n",
            "Batch 343 completed, Last reward: -5.84 Average reward: -6.23\n",
            "Batch 344 completed, Last reward: -6.63 Average reward: -5.90\n",
            "Batch 345 completed, Last reward: -6.78 Average reward: -6.05\n",
            "Batch 346 completed, Last reward: -7.98 Average reward: -6.58\n",
            "Batch 347 completed, Last reward: -7.13 Average reward: -5.99\n",
            "Batch 348 completed, Last reward: -6.16 Average reward: -6.10\n",
            "Batch 349 completed, Last reward: -5.83 Average reward: -6.23\n",
            "Batch 350 completed, Last reward: -7.45 Average reward: -6.19\n",
            "Batch 351 completed, Last reward: -5.50 Average reward: -6.35\n",
            "Batch 352 completed, Last reward: -5.63 Average reward: -6.35\n",
            "Batch 353 completed, Last reward: -6.71 Average reward: -6.05\n",
            "Batch 354 completed, Last reward: -7.88 Average reward: -6.35\n",
            "Batch 355 completed, Last reward: -7.05 Average reward: -5.87\n",
            "Batch 356 completed, Last reward: -7.00 Average reward: -6.51\n",
            "Batch 357 completed, Last reward: -7.38 Average reward: -5.86\n",
            "Batch 358 completed, Last reward: -4.93 Average reward: -6.13\n",
            "Batch 359 completed, Last reward: -6.02 Average reward: -6.03\n",
            "Batch 360 completed, Last reward: -6.74 Average reward: -6.21\n",
            "Batch 361 completed, Last reward: -8.27 Average reward: -6.14\n",
            "Batch 362 completed, Last reward: -5.73 Average reward: -6.21\n",
            "Batch 363 completed, Last reward: -9.60 Average reward: -6.44\n",
            "Batch 364 completed, Last reward: -6.52 Average reward: -6.29\n",
            "Batch 365 completed, Last reward: -7.06 Average reward: -6.61\n",
            "Batch 366 completed, Last reward: -5.66 Average reward: -6.39\n",
            "Batch 367 completed, Last reward: -6.40 Average reward: -6.28\n",
            "Batch 368 completed, Last reward: -8.27 Average reward: -6.12\n",
            "Batch 369 completed, Last reward: -5.68 Average reward: -6.23\n",
            "Batch 370 completed, Last reward: -7.50 Average reward: -6.06\n",
            "Batch 371 completed, Last reward: -9.29 Average reward: -6.52\n",
            "Batch 372 completed, Last reward: -6.43 Average reward: -6.33\n",
            "Batch 373 completed, Last reward: -5.90 Average reward: -6.43\n",
            "Batch 374 completed, Last reward: -6.17 Average reward: -6.11\n",
            "Batch 375 completed, Last reward: -6.14 Average reward: -6.39\n",
            "Batch 376 completed, Last reward: -7.72 Average reward: -5.98\n",
            "Batch 377 completed, Last reward: -8.37 Average reward: -6.58\n",
            "Batch 378 completed, Last reward: -7.19 Average reward: -6.02\n",
            "Batch 379 completed, Last reward: -8.05 Average reward: -6.30\n",
            "Batch 380 completed, Last reward: -6.70 Average reward: -6.15\n",
            "Batch 381 completed, Last reward: -9.24 Average reward: -6.29\n",
            "Batch 382 completed, Last reward: -6.85 Average reward: -6.19\n",
            "Batch 383 completed, Last reward: -5.26 Average reward: -6.05\n",
            "Batch 384 completed, Last reward: -6.57 Average reward: -6.60\n",
            "Batch 385 completed, Last reward: -5.29 Average reward: -6.34\n",
            "Batch 386 completed, Last reward: -5.63 Average reward: -6.03\n",
            "Batch 387 completed, Last reward: -6.11 Average reward: -6.00\n",
            "Batch 388 completed, Last reward: -6.22 Average reward: -5.97\n",
            "Batch 389 completed, Last reward: -8.84 Average reward: -5.95\n",
            "Batch 390 completed, Last reward: -7.04 Average reward: -6.60\n",
            "Batch 391 completed, Last reward: -5.20 Average reward: -6.07\n",
            "Batch 392 completed, Last reward: -7.32 Average reward: -6.39\n",
            "Batch 393 completed, Last reward: -6.21 Average reward: -6.25\n",
            "Batch 394 completed, Last reward: -4.89 Average reward: -5.91\n",
            "Batch 395 completed, Last reward: -8.13 Average reward: -6.10\n",
            "Batch 396 completed, Last reward: -6.37 Average reward: -6.44\n",
            "Batch 397 completed, Last reward: -6.06 Average reward: -6.29\n",
            "Batch 398 completed, Last reward: -6.79 Average reward: -6.39\n",
            "Batch 399 completed, Last reward: -7.27 Average reward: -6.47\n",
            "Batch 400 completed, Last reward: -7.02 Average reward: -5.98\n",
            "Batch 401 completed, Last reward: -7.44 Average reward: -6.24\n",
            "Batch 402 completed, Last reward: -7.46 Average reward: -6.21\n",
            "Batch 403 completed, Last reward: -6.55 Average reward: -6.50\n",
            "Batch 404 completed, Last reward: -5.85 Average reward: -6.17\n",
            "Batch 405 completed, Last reward: -8.63 Average reward: -6.25\n",
            "Batch 406 completed, Last reward: -6.67 Average reward: -6.52\n",
            "Batch 407 completed, Last reward: -6.63 Average reward: -6.15\n",
            "Batch 408 completed, Last reward: -6.46 Average reward: -6.14\n",
            "Batch 409 completed, Last reward: -8.14 Average reward: -6.20\n",
            "Batch 410 completed, Last reward: -7.16 Average reward: -6.19\n",
            "Batch 411 completed, Last reward: -5.94 Average reward: -6.51\n",
            "Batch 412 completed, Last reward: -7.03 Average reward: -6.09\n",
            "Batch 413 completed, Last reward: -5.93 Average reward: -5.95\n",
            "Batch 414 completed, Last reward: -5.86 Average reward: -6.23\n",
            "Batch 415 completed, Last reward: -5.41 Average reward: -6.18\n",
            "Batch 416 completed, Last reward: -7.67 Average reward: -6.31\n",
            "Batch 417 completed, Last reward: -6.10 Average reward: -6.09\n",
            "Batch 418 completed, Last reward: -7.78 Average reward: -6.19\n",
            "Batch 419 completed, Last reward: -5.03 Average reward: -6.11\n",
            "Batch 420 completed, Last reward: -7.17 Average reward: -6.39\n",
            "Batch 421 completed, Last reward: -7.17 Average reward: -6.36\n",
            "Batch 422 completed, Last reward: -6.94 Average reward: -6.10\n",
            "Batch 423 completed, Last reward: -5.06 Average reward: -6.53\n",
            "Batch 424 completed, Last reward: -6.06 Average reward: -6.42\n",
            "Batch 425 completed, Last reward: -6.90 Average reward: -6.33\n",
            "Batch 426 completed, Last reward: -8.05 Average reward: -6.22\n",
            "Batch 427 completed, Last reward: -6.22 Average reward: -6.18\n",
            "Batch 428 completed, Last reward: -6.18 Average reward: -5.84\n",
            "Batch 429 completed, Last reward: -5.71 Average reward: -6.10\n",
            "Batch 430 completed, Last reward: -6.96 Average reward: -6.05\n",
            "Batch 431 completed, Last reward: -6.04 Average reward: -6.30\n",
            "Batch 432 completed, Last reward: -8.36 Average reward: -6.13\n",
            "Batch 433 completed, Last reward: -6.38 Average reward: -6.13\n",
            "Batch 434 completed, Last reward: -6.95 Average reward: -6.46\n",
            "Batch 435 completed, Last reward: -7.34 Average reward: -6.14\n",
            "Batch 436 completed, Last reward: -6.35 Average reward: -6.12\n",
            "Batch 437 completed, Last reward: -5.43 Average reward: -6.29\n",
            "Batch 438 completed, Last reward: -7.75 Average reward: -6.04\n",
            "Batch 439 completed, Last reward: -8.31 Average reward: -6.52\n",
            "Batch 440 completed, Last reward: -7.92 Average reward: -6.19\n",
            "Batch 441 completed, Last reward: -9.88 Average reward: -6.45\n",
            "Batch 442 completed, Last reward: -8.85 Average reward: -6.22\n",
            "Batch 443 completed, Last reward: -8.22 Average reward: -6.61\n",
            "Batch 444 completed, Last reward: -6.33 Average reward: -6.06\n",
            "Batch 445 completed, Last reward: -7.67 Average reward: -6.35\n",
            "Batch 446 completed, Last reward: -5.43 Average reward: -6.22\n",
            "Batch 447 completed, Last reward: -7.57 Average reward: -6.23\n",
            "Batch 448 completed, Last reward: -5.55 Average reward: -6.05\n",
            "Batch 449 completed, Last reward: -4.97 Average reward: -6.28\n",
            "Batch 450 completed, Last reward: -6.22 Average reward: -6.11\n",
            "Batch 451 completed, Last reward: -6.84 Average reward: -6.20\n",
            "Batch 452 completed, Last reward: -6.19 Average reward: -6.42\n",
            "Batch 453 completed, Last reward: -7.48 Average reward: -6.31\n",
            "Batch 454 completed, Last reward: -5.45 Average reward: -6.39\n",
            "Batch 455 completed, Last reward: -6.03 Average reward: -6.34\n",
            "Batch 456 completed, Last reward: -5.23 Average reward: -6.37\n",
            "Batch 457 completed, Last reward: -4.64 Average reward: -6.34\n",
            "Batch 458 completed, Last reward: -6.12 Average reward: -6.18\n",
            "Batch 459 completed, Last reward: -6.59 Average reward: -6.18\n",
            "Batch 460 completed, Last reward: -6.60 Average reward: -6.01\n",
            "Batch 461 completed, Last reward: -7.20 Average reward: -6.07\n",
            "Batch 462 completed, Last reward: -7.56 Average reward: -6.33\n",
            "Batch 463 completed, Last reward: -7.80 Average reward: -5.93\n",
            "Batch 464 completed, Last reward: -7.11 Average reward: -6.66\n",
            "Batch 465 completed, Last reward: -7.02 Average reward: -6.47\n",
            "Batch 466 completed, Last reward: -7.89 Average reward: -6.19\n",
            "Batch 467 completed, Last reward: -7.11 Average reward: -6.35\n",
            "Batch 468 completed, Last reward: -6.71 Average reward: -6.48\n",
            "Batch 469 completed, Last reward: -7.56 Average reward: -6.36\n",
            "Batch 470 completed, Last reward: -6.81 Average reward: -6.25\n",
            "Batch 471 completed, Last reward: -9.23 Average reward: -6.09\n",
            "Batch 472 completed, Last reward: -6.74 Average reward: -6.29\n",
            "Batch 473 completed, Last reward: -6.31 Average reward: -6.37\n",
            "Batch 474 completed, Last reward: -7.15 Average reward: -6.30\n",
            "Batch 475 completed, Last reward: -8.59 Average reward: -6.39\n",
            "Batch 476 completed, Last reward: -5.93 Average reward: -6.12\n",
            "Batch 477 completed, Last reward: -6.40 Average reward: -6.05\n",
            "Batch 478 completed, Last reward: -7.11 Average reward: -6.13\n",
            "Batch 479 completed, Last reward: -7.96 Average reward: -6.23\n",
            "Batch 480 completed, Last reward: -5.82 Average reward: -6.11\n",
            "Batch 481 completed, Last reward: -9.04 Average reward: -6.07\n",
            "Batch 482 completed, Last reward: -5.91 Average reward: -6.18\n",
            "Batch 483 completed, Last reward: -6.08 Average reward: -6.38\n",
            "Batch 484 completed, Last reward: -7.08 Average reward: -6.27\n",
            "Batch 485 completed, Last reward: -9.20 Average reward: -6.38\n",
            "Batch 486 completed, Last reward: -7.22 Average reward: -6.15\n",
            "Batch 487 completed, Last reward: -6.37 Average reward: -6.37\n",
            "Batch 488 completed, Last reward: -7.19 Average reward: -6.47\n",
            "Batch 489 completed, Last reward: -6.23 Average reward: -6.26\n",
            "Batch 490 completed, Last reward: -6.62 Average reward: -6.35\n",
            "Batch 491 completed, Last reward: -5.63 Average reward: -6.03\n",
            "Batch 492 completed, Last reward: -7.17 Average reward: -6.19\n",
            "Batch 493 completed, Last reward: -6.62 Average reward: -6.46\n",
            "Batch 494 completed, Last reward: -7.71 Average reward: -6.06\n",
            "Batch 495 completed, Last reward: -6.77 Average reward: -5.93\n",
            "Batch 496 completed, Last reward: -6.68 Average reward: -6.38\n",
            "Batch 497 completed, Last reward: -6.38 Average reward: -6.08\n",
            "Batch 498 completed, Last reward: -5.81 Average reward: -6.23\n",
            "Batch 499 completed, Last reward: -6.11 Average reward: -6.05\n",
            "Batch 500 completed, Last reward: -5.14 Average reward: -6.34\n",
            "Batch 501 completed, Last reward: -8.06 Average reward: -6.28\n",
            "Batch 502 completed, Last reward: -5.35 Average reward: -6.33\n",
            "Batch 503 completed, Last reward: -5.54 Average reward: -6.13\n",
            "Batch 504 completed, Last reward: -8.58 Average reward: -6.11\n",
            "Batch 505 completed, Last reward: -9.16 Average reward: -6.41\n",
            "Batch 506 completed, Last reward: -5.31 Average reward: -6.32\n",
            "Batch 507 completed, Last reward: -5.84 Average reward: -6.09\n",
            "Batch 508 completed, Last reward: -6.83 Average reward: -6.26\n",
            "Batch 509 completed, Last reward: -7.51 Average reward: -6.09\n",
            "Batch 510 completed, Last reward: -5.42 Average reward: -6.07\n",
            "Batch 511 completed, Last reward: -7.08 Average reward: -5.88\n",
            "Batch 512 completed, Last reward: -6.97 Average reward: -6.27\n",
            "Batch 513 completed, Last reward: -6.42 Average reward: -5.85\n",
            "Batch 514 completed, Last reward: -5.33 Average reward: -6.46\n",
            "Batch 515 completed, Last reward: -7.19 Average reward: -6.16\n",
            "Batch 516 completed, Last reward: -8.25 Average reward: -6.52\n",
            "Batch 517 completed, Last reward: -6.03 Average reward: -6.19\n",
            "Batch 518 completed, Last reward: -5.52 Average reward: -6.02\n",
            "Batch 519 completed, Last reward: -6.98 Average reward: -5.99\n",
            "Batch 520 completed, Last reward: -7.34 Average reward: -6.08\n",
            "Batch 521 completed, Last reward: -10.61 Average reward: -6.38\n",
            "Batch 522 completed, Last reward: -8.38 Average reward: -6.14\n",
            "Batch 523 completed, Last reward: -7.35 Average reward: -6.26\n",
            "Batch 524 completed, Last reward: -5.06 Average reward: -6.19\n",
            "Batch 525 completed, Last reward: -5.60 Average reward: -5.96\n",
            "Batch 526 completed, Last reward: -6.68 Average reward: -6.03\n",
            "Batch 527 completed, Last reward: -8.31 Average reward: -6.31\n",
            "Batch 528 completed, Last reward: -5.54 Average reward: -5.91\n",
            "Batch 529 completed, Last reward: -8.47 Average reward: -6.57\n",
            "Batch 530 completed, Last reward: -6.15 Average reward: -5.93\n",
            "Batch 531 completed, Last reward: -10.06 Average reward: -6.36\n",
            "Batch 532 completed, Last reward: -5.82 Average reward: -6.07\n",
            "Batch 533 completed, Last reward: -6.67 Average reward: -5.95\n",
            "Batch 534 completed, Last reward: -5.76 Average reward: -6.36\n",
            "Batch 535 completed, Last reward: -5.69 Average reward: -6.40\n",
            "Batch 536 completed, Last reward: -3.29 Average reward: -5.91\n",
            "Batch 537 completed, Last reward: -6.73 Average reward: -6.08\n",
            "Batch 538 completed, Last reward: -6.25 Average reward: -6.44\n",
            "Batch 539 completed, Last reward: -6.56 Average reward: -6.34\n",
            "Batch 540 completed, Last reward: -5.27 Average reward: -6.18\n",
            "Batch 541 completed, Last reward: -7.33 Average reward: -6.33\n",
            "Batch 542 completed, Last reward: -6.96 Average reward: -6.32\n",
            "Batch 543 completed, Last reward: -8.17 Average reward: -6.09\n",
            "Batch 544 completed, Last reward: -6.67 Average reward: -6.68\n",
            "Batch 545 completed, Last reward: -7.48 Average reward: -6.20\n",
            "Batch 546 completed, Last reward: -6.40 Average reward: -6.09\n",
            "Batch 547 completed, Last reward: -5.96 Average reward: -6.36\n",
            "Batch 548 completed, Last reward: -7.14 Average reward: -6.15\n",
            "Batch 549 completed, Last reward: -6.37 Average reward: -6.25\n",
            "Batch 550 completed, Last reward: -6.34 Average reward: -6.09\n",
            "Batch 551 completed, Last reward: -5.98 Average reward: -6.22\n",
            "Batch 552 completed, Last reward: -6.63 Average reward: -6.27\n",
            "Batch 553 completed, Last reward: -5.87 Average reward: -6.20\n",
            "Batch 554 completed, Last reward: -7.79 Average reward: -6.12\n",
            "Batch 555 completed, Last reward: -7.91 Average reward: -6.49\n",
            "Batch 556 completed, Last reward: -9.24 Average reward: -6.10\n",
            "Batch 557 completed, Last reward: -6.38 Average reward: -6.35\n",
            "Batch 558 completed, Last reward: -4.93 Average reward: -5.92\n",
            "Batch 559 completed, Last reward: -6.19 Average reward: -6.31\n",
            "Batch 560 completed, Last reward: -5.98 Average reward: -6.11\n",
            "Batch 561 completed, Last reward: -8.78 Average reward: -6.55\n",
            "Batch 562 completed, Last reward: -7.28 Average reward: -6.32\n",
            "Batch 563 completed, Last reward: -7.38 Average reward: -6.47\n",
            "Batch 564 completed, Last reward: -6.29 Average reward: -6.21\n",
            "Batch 565 completed, Last reward: -9.59 Average reward: -6.31\n",
            "Batch 566 completed, Last reward: -5.01 Average reward: -6.33\n",
            "Batch 567 completed, Last reward: -5.09 Average reward: -6.42\n",
            "Batch 568 completed, Last reward: -6.38 Average reward: -6.17\n",
            "Batch 569 completed, Last reward: -7.65 Average reward: -6.42\n",
            "Batch 570 completed, Last reward: -7.97 Average reward: -6.40\n",
            "Batch 571 completed, Last reward: -8.60 Average reward: -6.28\n",
            "Batch 572 completed, Last reward: -5.71 Average reward: -6.43\n",
            "Batch 573 completed, Last reward: -6.67 Average reward: -6.31\n",
            "Batch 574 completed, Last reward: -5.00 Average reward: -6.12\n",
            "Batch 575 completed, Last reward: -6.14 Average reward: -6.21\n",
            "Batch 576 completed, Last reward: -7.06 Average reward: -6.09\n",
            "Batch 577 completed, Last reward: -6.90 Average reward: -6.33\n",
            "Batch 578 completed, Last reward: -6.87 Average reward: -5.99\n",
            "Batch 579 completed, Last reward: -5.97 Average reward: -6.32\n",
            "Batch 580 completed, Last reward: -6.70 Average reward: -6.38\n",
            "Batch 581 completed, Last reward: -8.23 Average reward: -6.29\n",
            "Batch 582 completed, Last reward: -6.48 Average reward: -6.26\n",
            "Batch 583 completed, Last reward: -6.19 Average reward: -6.26\n",
            "Batch 584 completed, Last reward: -5.98 Average reward: -6.31\n",
            "Batch 585 completed, Last reward: -6.06 Average reward: -6.19\n",
            "Batch 586 completed, Last reward: -6.88 Average reward: -6.40\n",
            "Batch 587 completed, Last reward: -6.48 Average reward: -6.44\n",
            "Batch 588 completed, Last reward: -7.58 Average reward: -6.12\n",
            "Batch 589 completed, Last reward: -7.91 Average reward: -6.21\n",
            "Batch 590 completed, Last reward: -6.33 Average reward: -6.18\n",
            "Batch 591 completed, Last reward: -8.20 Average reward: -6.19\n",
            "Batch 592 completed, Last reward: -8.31 Average reward: -6.13\n",
            "Batch 593 completed, Last reward: -6.45 Average reward: -6.12\n",
            "Batch 594 completed, Last reward: -5.99 Average reward: -5.97\n",
            "Batch 595 completed, Last reward: -7.44 Average reward: -5.90\n",
            "Batch 596 completed, Last reward: -5.62 Average reward: -5.91\n",
            "Batch 597 completed, Last reward: -6.33 Average reward: -6.24\n",
            "Batch 598 completed, Last reward: -8.51 Average reward: -6.01\n",
            "Batch 599 completed, Last reward: -8.22 Average reward: -6.50\n",
            "Batch 600 completed, Last reward: -7.76 Average reward: -6.24\n",
            "Batch 601 completed, Last reward: -5.60 Average reward: -5.89\n",
            "Batch 602 completed, Last reward: -5.91 Average reward: -6.28\n",
            "Batch 603 completed, Last reward: -5.38 Average reward: -6.14\n",
            "Batch 604 completed, Last reward: -5.96 Average reward: -6.25\n",
            "Batch 605 completed, Last reward: -6.68 Average reward: -6.48\n",
            "Batch 606 completed, Last reward: -5.92 Average reward: -6.01\n",
            "Batch 607 completed, Last reward: -6.65 Average reward: -6.31\n",
            "Batch 608 completed, Last reward: -8.91 Average reward: -6.31\n",
            "Batch 609 completed, Last reward: -10.98 Average reward: -6.35\n",
            "Batch 610 completed, Last reward: -5.21 Average reward: -6.37\n",
            "Batch 611 completed, Last reward: -6.92 Average reward: -6.56\n",
            "Batch 612 completed, Last reward: -6.94 Average reward: -5.98\n",
            "Batch 613 completed, Last reward: -5.36 Average reward: -5.85\n",
            "Batch 614 completed, Last reward: -7.41 Average reward: -6.35\n",
            "Batch 615 completed, Last reward: -5.20 Average reward: -6.52\n",
            "Batch 616 completed, Last reward: -7.50 Average reward: -6.31\n",
            "Batch 617 completed, Last reward: -6.62 Average reward: -6.25\n",
            "Batch 618 completed, Last reward: -6.49 Average reward: -6.31\n",
            "Batch 619 completed, Last reward: -6.71 Average reward: -6.23\n",
            "Batch 620 completed, Last reward: -7.23 Average reward: -6.15\n",
            "Batch 621 completed, Last reward: -7.60 Average reward: -6.47\n",
            "Batch 622 completed, Last reward: -8.29 Average reward: -6.09\n",
            "Batch 623 completed, Last reward: -6.59 Average reward: -6.17\n",
            "Batch 624 completed, Last reward: -5.69 Average reward: -6.13\n",
            "Batch 625 completed, Last reward: -7.42 Average reward: -6.15\n",
            "Batch 626 completed, Last reward: -5.44 Average reward: -6.02\n",
            "Batch 627 completed, Last reward: -4.78 Average reward: -6.19\n",
            "Batch 628 completed, Last reward: -6.27 Average reward: -5.89\n",
            "Batch 629 completed, Last reward: -7.79 Average reward: -6.61\n",
            "Batch 630 completed, Last reward: -5.99 Average reward: -6.30\n",
            "Batch 631 completed, Last reward: -7.79 Average reward: -6.26\n",
            "Batch 632 completed, Last reward: -9.30 Average reward: -6.15\n",
            "Batch 633 completed, Last reward: -5.65 Average reward: -6.09\n",
            "Batch 634 completed, Last reward: -6.63 Average reward: -6.04\n",
            "Batch 635 completed, Last reward: -8.39 Average reward: -6.68\n",
            "Batch 636 completed, Last reward: -5.48 Average reward: -5.89\n",
            "Batch 637 completed, Last reward: -6.21 Average reward: -6.29\n",
            "Batch 638 completed, Last reward: -6.35 Average reward: -6.46\n",
            "Batch 639 completed, Last reward: -7.44 Average reward: -6.16\n",
            "Batch 640 completed, Last reward: -5.77 Average reward: -6.15\n",
            "Batch 641 completed, Last reward: -7.55 Average reward: -6.67\n",
            "Batch 642 completed, Last reward: -7.60 Average reward: -5.90\n",
            "Batch 643 completed, Last reward: -7.58 Average reward: -6.28\n",
            "Batch 644 completed, Last reward: -6.35 Average reward: -6.12\n",
            "Batch 645 completed, Last reward: -5.77 Average reward: -6.01\n",
            "Batch 646 completed, Last reward: -8.73 Average reward: -6.21\n",
            "Batch 647 completed, Last reward: -7.91 Average reward: -6.24\n",
            "Batch 648 completed, Last reward: -7.30 Average reward: -6.51\n",
            "Batch 649 completed, Last reward: -6.19 Average reward: -6.19\n",
            "Batch 650 completed, Last reward: -6.23 Average reward: -6.28\n",
            "Batch 651 completed, Last reward: -7.32 Average reward: -6.14\n",
            "Batch 652 completed, Last reward: -6.05 Average reward: -6.34\n",
            "Batch 653 completed, Last reward: -9.15 Average reward: -6.37\n",
            "Batch 654 completed, Last reward: -5.58 Average reward: -6.46\n",
            "Batch 655 completed, Last reward: -7.31 Average reward: -6.06\n",
            "Batch 656 completed, Last reward: -10.38 Average reward: -6.15\n",
            "Batch 657 completed, Last reward: -6.65 Average reward: -6.03\n",
            "Batch 658 completed, Last reward: -6.69 Average reward: -6.66\n",
            "Batch 659 completed, Last reward: -6.57 Average reward: -6.19\n",
            "Batch 660 completed, Last reward: -6.01 Average reward: -6.19\n",
            "Batch 661 completed, Last reward: -7.69 Average reward: -6.31\n",
            "Batch 662 completed, Last reward: -9.81 Average reward: -6.06\n",
            "Batch 663 completed, Last reward: -8.88 Average reward: -5.87\n",
            "Batch 664 completed, Last reward: -8.49 Average reward: -6.34\n",
            "Batch 665 completed, Last reward: -6.31 Average reward: -6.12\n",
            "Batch 666 completed, Last reward: -7.15 Average reward: -6.62\n",
            "Batch 667 completed, Last reward: -6.87 Average reward: -6.08\n",
            "Batch 668 completed, Last reward: -6.04 Average reward: -6.32\n",
            "Batch 669 completed, Last reward: -5.84 Average reward: -6.13\n",
            "Batch 670 completed, Last reward: -6.74 Average reward: -6.24\n",
            "Batch 671 completed, Last reward: -6.63 Average reward: -6.38\n",
            "Batch 672 completed, Last reward: -6.62 Average reward: -6.22\n",
            "Batch 673 completed, Last reward: -5.91 Average reward: -6.38\n",
            "Batch 674 completed, Last reward: -7.49 Average reward: -6.53\n",
            "Batch 675 completed, Last reward: -7.49 Average reward: -6.18\n",
            "Batch 676 completed, Last reward: -5.52 Average reward: -6.24\n",
            "Batch 677 completed, Last reward: -6.35 Average reward: -6.39\n",
            "Batch 678 completed, Last reward: -7.04 Average reward: -6.23\n",
            "Batch 679 completed, Last reward: -5.37 Average reward: -6.18\n",
            "Batch 680 completed, Last reward: -6.11 Average reward: -6.25\n",
            "Batch 681 completed, Last reward: -7.79 Average reward: -6.10\n",
            "Batch 682 completed, Last reward: -4.78 Average reward: -6.08\n",
            "Batch 683 completed, Last reward: -6.17 Average reward: -6.63\n",
            "Batch 684 completed, Last reward: -8.79 Average reward: -6.22\n",
            "Batch 685 completed, Last reward: -7.00 Average reward: -6.22\n",
            "Batch 686 completed, Last reward: -7.66 Average reward: -6.56\n",
            "Batch 687 completed, Last reward: -6.45 Average reward: -6.07\n",
            "Batch 688 completed, Last reward: -7.80 Average reward: -6.29\n",
            "Batch 689 completed, Last reward: -9.05 Average reward: -6.49\n",
            "Batch 690 completed, Last reward: -6.86 Average reward: -6.07\n",
            "Batch 691 completed, Last reward: -6.74 Average reward: -6.33\n",
            "Batch 692 completed, Last reward: -6.53 Average reward: -6.29\n",
            "Batch 693 completed, Last reward: -5.11 Average reward: -6.13\n",
            "Batch 694 completed, Last reward: -7.62 Average reward: -6.37\n",
            "Batch 695 completed, Last reward: -7.92 Average reward: -6.51\n",
            "Batch 696 completed, Last reward: -7.47 Average reward: -6.15\n",
            "Batch 697 completed, Last reward: -6.67 Average reward: -6.64\n",
            "Batch 698 completed, Last reward: -7.64 Average reward: -6.28\n",
            "Batch 699 completed, Last reward: -5.76 Average reward: -6.28\n",
            "Batch 700 completed, Last reward: -8.92 Average reward: -6.22\n",
            "Batch 701 completed, Last reward: -6.69 Average reward: -5.96\n",
            "Batch 702 completed, Last reward: -6.25 Average reward: -6.24\n",
            "Batch 703 completed, Last reward: -7.60 Average reward: -6.19\n",
            "Batch 704 completed, Last reward: -7.29 Average reward: -6.24\n",
            "Batch 705 completed, Last reward: -6.02 Average reward: -6.46\n",
            "Batch 706 completed, Last reward: -5.89 Average reward: -6.23\n",
            "Batch 707 completed, Last reward: -7.51 Average reward: -6.41\n",
            "Batch 708 completed, Last reward: -8.18 Average reward: -6.31\n",
            "Batch 709 completed, Last reward: -5.15 Average reward: -6.12\n",
            "Batch 710 completed, Last reward: -6.03 Average reward: -6.19\n",
            "Batch 711 completed, Last reward: -5.21 Average reward: -6.01\n",
            "Batch 712 completed, Last reward: -6.84 Average reward: -5.89\n",
            "Batch 713 completed, Last reward: -6.55 Average reward: -6.19\n",
            "Batch 714 completed, Last reward: -7.44 Average reward: -6.00\n",
            "Batch 715 completed, Last reward: -8.03 Average reward: -6.22\n",
            "Batch 716 completed, Last reward: -7.45 Average reward: -6.19\n",
            "Batch 717 completed, Last reward: -6.48 Average reward: -6.30\n",
            "Batch 718 completed, Last reward: -4.88 Average reward: -6.38\n",
            "Batch 719 completed, Last reward: -6.95 Average reward: -6.60\n",
            "Batch 720 completed, Last reward: -8.08 Average reward: -6.27\n",
            "Batch 721 completed, Last reward: -6.21 Average reward: -6.29\n",
            "Batch 722 completed, Last reward: -5.86 Average reward: -6.45\n",
            "Batch 723 completed, Last reward: -6.87 Average reward: -6.04\n",
            "Batch 724 completed, Last reward: -4.99 Average reward: -6.03\n",
            "Batch 725 completed, Last reward: -5.35 Average reward: -6.10\n",
            "Batch 726 completed, Last reward: -5.32 Average reward: -6.19\n",
            "Batch 727 completed, Last reward: -7.61 Average reward: -6.28\n",
            "Batch 728 completed, Last reward: -7.32 Average reward: -6.36\n",
            "Batch 729 completed, Last reward: -5.77 Average reward: -6.01\n",
            "Batch 730 completed, Last reward: -7.44 Average reward: -6.12\n",
            "Batch 731 completed, Last reward: -7.24 Average reward: -6.29\n",
            "Batch 732 completed, Last reward: -7.91 Average reward: -6.56\n",
            "Batch 733 completed, Last reward: -5.83 Average reward: -6.18\n",
            "Batch 734 completed, Last reward: -6.45 Average reward: -6.08\n",
            "Batch 735 completed, Last reward: -7.05 Average reward: -6.26\n",
            "Batch 736 completed, Last reward: -6.58 Average reward: -6.22\n",
            "Batch 737 completed, Last reward: -6.64 Average reward: -6.25\n",
            "Batch 738 completed, Last reward: -6.40 Average reward: -6.18\n",
            "Batch 739 completed, Last reward: -6.10 Average reward: -6.08\n",
            "Batch 740 completed, Last reward: -9.21 Average reward: -6.42\n",
            "Batch 741 completed, Last reward: -6.76 Average reward: -6.06\n",
            "Batch 742 completed, Last reward: -7.24 Average reward: -6.18\n",
            "Batch 743 completed, Last reward: -5.37 Average reward: -6.11\n",
            "Batch 744 completed, Last reward: -7.49 Average reward: -6.31\n",
            "Batch 745 completed, Last reward: -6.63 Average reward: -6.37\n",
            "Batch 746 completed, Last reward: -6.14 Average reward: -6.21\n",
            "Batch 747 completed, Last reward: -7.35 Average reward: -6.26\n",
            "Batch 748 completed, Last reward: -6.78 Average reward: -6.44\n",
            "Batch 749 completed, Last reward: -5.33 Average reward: -6.00\n",
            "Batch 750 completed, Last reward: -5.78 Average reward: -6.13\n",
            "Batch 751 completed, Last reward: -7.55 Average reward: -6.42\n",
            "Batch 752 completed, Last reward: -7.72 Average reward: -6.15\n",
            "Batch 753 completed, Last reward: -5.62 Average reward: -5.97\n",
            "Batch 754 completed, Last reward: -6.56 Average reward: -6.02\n",
            "Batch 755 completed, Last reward: -6.47 Average reward: -6.27\n",
            "Batch 756 completed, Last reward: -8.42 Average reward: -6.29\n",
            "Batch 757 completed, Last reward: -7.10 Average reward: -6.25\n",
            "Batch 758 completed, Last reward: -8.60 Average reward: -6.27\n",
            "Batch 759 completed, Last reward: -6.91 Average reward: -6.23\n",
            "Batch 760 completed, Last reward: -7.37 Average reward: -6.26\n",
            "Batch 761 completed, Last reward: -4.74 Average reward: -6.22\n",
            "Batch 762 completed, Last reward: -5.68 Average reward: -6.41\n",
            "Batch 763 completed, Last reward: -6.85 Average reward: -6.31\n",
            "Batch 764 completed, Last reward: -6.50 Average reward: -5.95\n",
            "Batch 765 completed, Last reward: -8.27 Average reward: -6.59\n",
            "Batch 766 completed, Last reward: -5.84 Average reward: -6.04\n",
            "Batch 767 completed, Last reward: -7.14 Average reward: -6.13\n",
            "Batch 768 completed, Last reward: -8.48 Average reward: -6.29\n",
            "Batch 769 completed, Last reward: -7.92 Average reward: -6.49\n",
            "Batch 770 completed, Last reward: -6.92 Average reward: -6.25\n",
            "Batch 771 completed, Last reward: -5.79 Average reward: -6.33\n",
            "Batch 772 completed, Last reward: -6.24 Average reward: -6.06\n",
            "Batch 773 completed, Last reward: -6.97 Average reward: -6.08\n",
            "Batch 774 completed, Last reward: -6.54 Average reward: -6.07\n",
            "Batch 775 completed, Last reward: -4.48 Average reward: -6.47\n",
            "Batch 776 completed, Last reward: -6.95 Average reward: -6.13\n",
            "Batch 777 completed, Last reward: -5.39 Average reward: -6.36\n",
            "Batch 778 completed, Last reward: -6.69 Average reward: -6.31\n",
            "Batch 779 completed, Last reward: -5.79 Average reward: -6.24\n",
            "Batch 780 completed, Last reward: -6.45 Average reward: -6.20\n",
            "Batch 781 completed, Last reward: -5.72 Average reward: -6.15\n",
            "Batch 782 completed, Last reward: -6.21 Average reward: -6.58\n",
            "Batch 783 completed, Last reward: -7.63 Average reward: -6.47\n",
            "Batch 784 completed, Last reward: -7.34 Average reward: -6.24\n",
            "Batch 785 completed, Last reward: -5.86 Average reward: -6.05\n",
            "Batch 786 completed, Last reward: -8.06 Average reward: -6.34\n",
            "Batch 787 completed, Last reward: -7.64 Average reward: -6.30\n",
            "Batch 788 completed, Last reward: -9.86 Average reward: -6.21\n",
            "Batch 789 completed, Last reward: -5.99 Average reward: -6.30\n",
            "Batch 790 completed, Last reward: -6.93 Average reward: -6.12\n",
            "Batch 791 completed, Last reward: -5.11 Average reward: -6.31\n",
            "Batch 792 completed, Last reward: -6.48 Average reward: -6.09\n",
            "Batch 793 completed, Last reward: -4.95 Average reward: -6.27\n",
            "Batch 794 completed, Last reward: -7.91 Average reward: -6.18\n",
            "Batch 795 completed, Last reward: -5.01 Average reward: -6.11\n",
            "Batch 796 completed, Last reward: -7.75 Average reward: -6.04\n",
            "Batch 797 completed, Last reward: -6.61 Average reward: -6.44\n",
            "Batch 798 completed, Last reward: -7.07 Average reward: -6.52\n",
            "Batch 799 completed, Last reward: -6.53 Average reward: -6.20\n",
            "Batch 800 completed, Last reward: -6.82 Average reward: -6.20\n",
            "Batch 801 completed, Last reward: -5.87 Average reward: -6.42\n",
            "Batch 802 completed, Last reward: -7.21 Average reward: -6.25\n",
            "Batch 803 completed, Last reward: -7.35 Average reward: -6.23\n",
            "Batch 804 completed, Last reward: -7.71 Average reward: -6.07\n",
            "Batch 805 completed, Last reward: -7.23 Average reward: -6.22\n",
            "Batch 806 completed, Last reward: -7.39 Average reward: -6.10\n",
            "Batch 807 completed, Last reward: -8.57 Average reward: -6.34\n",
            "Batch 808 completed, Last reward: -8.98 Average reward: -6.59\n",
            "Batch 809 completed, Last reward: -6.44 Average reward: -6.17\n",
            "Batch 810 completed, Last reward: -5.93 Average reward: -6.20\n",
            "Batch 811 completed, Last reward: -7.23 Average reward: -6.16\n",
            "Batch 812 completed, Last reward: -5.57 Average reward: -6.02\n",
            "Batch 813 completed, Last reward: -5.97 Average reward: -6.33\n",
            "Batch 814 completed, Last reward: -6.09 Average reward: -6.35\n",
            "Batch 815 completed, Last reward: -6.80 Average reward: -6.35\n",
            "Batch 816 completed, Last reward: -6.05 Average reward: -6.25\n",
            "Batch 817 completed, Last reward: -8.06 Average reward: -6.19\n",
            "Batch 818 completed, Last reward: -6.07 Average reward: -6.42\n",
            "Batch 819 completed, Last reward: -7.08 Average reward: -6.00\n",
            "Batch 820 completed, Last reward: -6.78 Average reward: -5.98\n",
            "Batch 821 completed, Last reward: -5.56 Average reward: -6.08\n",
            "Batch 822 completed, Last reward: -6.73 Average reward: -6.32\n",
            "Batch 823 completed, Last reward: -7.82 Average reward: -6.13\n",
            "Batch 824 completed, Last reward: -7.98 Average reward: -6.24\n",
            "Batch 825 completed, Last reward: -5.98 Average reward: -6.07\n",
            "Batch 826 completed, Last reward: -6.86 Average reward: -6.19\n",
            "Batch 827 completed, Last reward: -6.66 Average reward: -6.19\n",
            "Batch 828 completed, Last reward: -5.95 Average reward: -6.23\n",
            "Batch 829 completed, Last reward: -5.65 Average reward: -5.77\n",
            "Batch 830 completed, Last reward: -5.17 Average reward: -6.18\n",
            "Batch 831 completed, Last reward: -7.38 Average reward: -6.06\n",
            "Batch 832 completed, Last reward: -6.24 Average reward: -5.86\n",
            "Batch 833 completed, Last reward: -5.89 Average reward: -6.32\n",
            "Batch 834 completed, Last reward: -9.37 Average reward: -6.43\n",
            "Batch 835 completed, Last reward: -7.66 Average reward: -6.25\n",
            "Batch 836 completed, Last reward: -6.37 Average reward: -6.19\n",
            "Batch 837 completed, Last reward: -6.92 Average reward: -6.20\n",
            "Batch 838 completed, Last reward: -7.57 Average reward: -6.37\n",
            "Batch 839 completed, Last reward: -8.07 Average reward: -6.31\n",
            "Batch 840 completed, Last reward: -6.25 Average reward: -6.10\n",
            "Batch 841 completed, Last reward: -6.70 Average reward: -6.11\n",
            "Batch 842 completed, Last reward: -6.00 Average reward: -6.08\n",
            "Batch 843 completed, Last reward: -6.85 Average reward: -6.24\n",
            "Batch 844 completed, Last reward: -6.66 Average reward: -6.38\n",
            "Batch 845 completed, Last reward: -8.25 Average reward: -6.11\n",
            "Batch 846 completed, Last reward: -6.14 Average reward: -6.55\n",
            "Batch 847 completed, Last reward: -8.47 Average reward: -6.22\n",
            "Batch 848 completed, Last reward: -6.33 Average reward: -6.36\n",
            "Batch 849 completed, Last reward: -6.97 Average reward: -6.01\n",
            "Batch 850 completed, Last reward: -6.88 Average reward: -6.03\n",
            "Batch 851 completed, Last reward: -5.88 Average reward: -6.42\n",
            "Batch 852 completed, Last reward: -8.08 Average reward: -6.46\n",
            "Batch 853 completed, Last reward: -8.14 Average reward: -6.33\n",
            "Batch 854 completed, Last reward: -7.99 Average reward: -6.43\n",
            "Batch 855 completed, Last reward: -6.80 Average reward: -6.34\n",
            "Batch 856 completed, Last reward: -9.34 Average reward: -6.59\n",
            "Batch 857 completed, Last reward: -9.49 Average reward: -6.24\n",
            "Batch 858 completed, Last reward: -6.20 Average reward: -5.98\n",
            "Batch 859 completed, Last reward: -7.72 Average reward: -6.32\n",
            "Batch 860 completed, Last reward: -7.36 Average reward: -6.22\n",
            "Batch 861 completed, Last reward: -5.33 Average reward: -6.05\n",
            "Batch 862 completed, Last reward: -6.25 Average reward: -6.63\n",
            "Batch 863 completed, Last reward: -6.52 Average reward: -5.94\n",
            "Batch 864 completed, Last reward: -6.71 Average reward: -6.08\n",
            "Batch 865 completed, Last reward: -5.79 Average reward: -6.30\n",
            "Batch 866 completed, Last reward: -4.80 Average reward: -5.87\n",
            "Batch 867 completed, Last reward: -6.71 Average reward: -6.07\n",
            "Batch 868 completed, Last reward: -5.64 Average reward: -6.08\n",
            "Batch 869 completed, Last reward: -6.33 Average reward: -6.35\n",
            "Batch 870 completed, Last reward: -6.00 Average reward: -6.13\n",
            "Batch 871 completed, Last reward: -5.64 Average reward: -6.36\n",
            "Batch 872 completed, Last reward: -7.43 Average reward: -6.29\n",
            "Batch 873 completed, Last reward: -10.56 Average reward: -6.31\n",
            "Batch 874 completed, Last reward: -5.26 Average reward: -6.05\n",
            "Batch 875 completed, Last reward: -6.94 Average reward: -6.33\n",
            "Batch 876 completed, Last reward: -7.94 Average reward: -6.24\n",
            "Batch 877 completed, Last reward: -5.94 Average reward: -6.28\n",
            "Batch 878 completed, Last reward: -6.50 Average reward: -6.24\n",
            "Batch 879 completed, Last reward: -7.39 Average reward: -6.21\n",
            "Batch 880 completed, Last reward: -7.03 Average reward: -6.20\n",
            "Batch 881 completed, Last reward: -6.84 Average reward: -6.22\n",
            "Batch 882 completed, Last reward: -8.05 Average reward: -6.31\n",
            "Batch 883 completed, Last reward: -7.18 Average reward: -6.41\n",
            "Batch 884 completed, Last reward: -8.53 Average reward: -6.25\n",
            "Batch 885 completed, Last reward: -8.56 Average reward: -6.55\n",
            "Batch 886 completed, Last reward: -8.55 Average reward: -6.16\n",
            "Batch 887 completed, Last reward: -8.95 Average reward: -6.19\n",
            "Batch 888 completed, Last reward: -6.84 Average reward: -5.95\n",
            "Batch 889 completed, Last reward: -5.77 Average reward: -6.44\n",
            "Batch 890 completed, Last reward: -5.85 Average reward: -5.99\n",
            "Batch 891 completed, Last reward: -6.13 Average reward: -6.54\n",
            "Batch 892 completed, Last reward: -9.18 Average reward: -6.42\n",
            "Batch 893 completed, Last reward: -6.77 Average reward: -6.26\n",
            "Batch 894 completed, Last reward: -7.67 Average reward: -6.65\n",
            "Batch 895 completed, Last reward: -8.26 Average reward: -6.55\n",
            "Batch 896 completed, Last reward: -7.53 Average reward: -6.43\n",
            "Batch 897 completed, Last reward: -8.64 Average reward: -6.37\n",
            "Batch 898 completed, Last reward: -7.51 Average reward: -6.17\n",
            "Batch 899 completed, Last reward: -6.10 Average reward: -6.07\n",
            "Batch 900 completed, Last reward: -7.97 Average reward: -6.45\n",
            "Batch 901 completed, Last reward: -7.27 Average reward: -6.40\n",
            "Batch 902 completed, Last reward: -8.04 Average reward: -5.98\n",
            "Batch 903 completed, Last reward: -5.95 Average reward: -6.21\n",
            "Batch 904 completed, Last reward: -6.10 Average reward: -6.08\n",
            "Batch 905 completed, Last reward: -7.49 Average reward: -6.05\n",
            "Batch 906 completed, Last reward: -7.43 Average reward: -6.07\n",
            "Batch 907 completed, Last reward: -5.97 Average reward: -6.05\n",
            "Batch 908 completed, Last reward: -5.15 Average reward: -6.12\n",
            "Batch 909 completed, Last reward: -4.95 Average reward: -6.19\n",
            "Batch 910 completed, Last reward: -5.52 Average reward: -5.99\n",
            "Batch 911 completed, Last reward: -5.95 Average reward: -6.24\n",
            "Batch 912 completed, Last reward: -6.89 Average reward: -6.28\n",
            "Batch 913 completed, Last reward: -5.20 Average reward: -6.02\n",
            "Batch 914 completed, Last reward: -8.66 Average reward: -6.59\n",
            "Batch 915 completed, Last reward: -7.04 Average reward: -6.40\n",
            "Batch 916 completed, Last reward: -4.74 Average reward: -6.33\n",
            "Batch 917 completed, Last reward: -6.44 Average reward: -6.22\n",
            "Batch 918 completed, Last reward: -7.00 Average reward: -6.18\n",
            "Batch 919 completed, Last reward: -7.23 Average reward: -6.33\n",
            "Batch 920 completed, Last reward: -7.83 Average reward: -6.34\n",
            "Batch 921 completed, Last reward: -8.23 Average reward: -6.34\n",
            "Batch 922 completed, Last reward: -7.74 Average reward: -6.21\n",
            "Batch 923 completed, Last reward: -7.67 Average reward: -6.54\n",
            "Batch 924 completed, Last reward: -6.24 Average reward: -6.23\n",
            "Batch 925 completed, Last reward: -7.17 Average reward: -6.29\n",
            "Batch 926 completed, Last reward: -8.41 Average reward: -6.29\n",
            "Batch 927 completed, Last reward: -4.95 Average reward: -6.56\n",
            "Batch 928 completed, Last reward: -6.08 Average reward: -6.18\n",
            "Batch 929 completed, Last reward: -7.01 Average reward: -6.26\n",
            "Batch 930 completed, Last reward: -7.55 Average reward: -6.36\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env.reset()\n",
        "total_timesteps = 0\n",
        "obs = copy.deepcopy(env.reset())\n",
        "\n",
        "policy.train(int(max_timesteps), batch_size, tau)\n",
        "\n",
        "evaluations.append(evaluate_policy(policy))\n",
        "if save_models: policy.save(\"%s\" % (file_name), directory=\"./pytorch_models\")\n",
        "np.save(\"./results/%s\" % (file_name), evaluations)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_mH4R4hb4dhn",
        "outputId": "b4fbd0f9-679b-4a05-cf5c-5216e4986b1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 0 completed, Last reward: -6.02 Average reward: -6.81\n",
            "Batch 1 completed, Last reward: -6.69 Average reward: -7.00\n",
            "Batch 2 completed, Last reward: -5.71 Average reward: -6.77\n",
            "Batch 3 completed, Last reward: -6.69 Average reward: -6.61\n",
            "Batch 4 completed, Last reward: -8.18 Average reward: -6.87\n",
            "Batch 5 completed, Last reward: -7.82 Average reward: -6.99\n",
            "Batch 6 completed, Last reward: -5.72 Average reward: -6.63\n",
            "Batch 7 completed, Last reward: -5.79 Average reward: -6.77\n",
            "Batch 8 completed, Last reward: -6.99 Average reward: -6.84\n",
            "Batch 9 completed, Last reward: -6.11 Average reward: -6.70\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: -6.614827\n",
            "---------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcB82cJup6kp"
      },
      "source": [
        "## The inference policy function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "NUIPg-FCp-fG"
      },
      "outputs": [],
      "source": [
        "def evaluate_final_policy(policy,random = 1, eval_episodes=1):\n",
        "  avg_reward = 0.\n",
        "  distance = 0.\n",
        "  for k in range(eval_episodes):\n",
        "    obs = env.reset(seed = random)\n",
        "    obs = torch.Tensor(obs.reshape(1,env.customer_count,state_dim)).to(device)\n",
        "    done = False\n",
        "    while not done:\n",
        "      action, current_Q = policy.select_target_action(obs, state_dim)\n",
        "      obs, reward, done = env.step(action)\n",
        "      obs = torch.Tensor(obs.reshape(1,env.customer_count,state_dim)).to(device)\n",
        "      avg_reward += reward\n",
        "    total_distance = 0\n",
        "    for j in range(len(env.routes)):\n",
        "      for i in range(len(env.routes[j])-1):\n",
        "        total_distance = total_distance + ((env.VRP[env.routes[j][i],0]-env.VRP[env.routes[j][i+1],0])**2+(env.VRP[env.routes[j][i],1]-env.VRP[env.routes[j][i+1],1])**2)**0.5\n",
        "    distance += total_distance\n",
        "  distance /= eval_episodes\n",
        "  print (\"---------------------------------------\")\n",
        "  print (\"Average Distance over the Evaluation Step: %f\" % (distance))\n",
        "  print (\"---------------------------------------\")\n",
        "  return distance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZbeSFVKo1rd"
      },
      "source": [
        "## Let's see if we can test the implementation !\n",
        "#I hope my code works in class too :)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "\n",
        "# Assuming evaluate_final_policy, policy, and env are already defined and initialized\n",
        "results = np.zeros(100)\n",
        "policy.load(file_name, './pytorch_models/')\n",
        "\n",
        "for i in range(100):\n",
        "    evaluations = [evaluate_final_policy(policy, random=i)]\n",
        "    results[i] = evaluations[0]\n",
        "\n",
        "print(results)\n",
        "\n",
        "def plot_average_distances(results):\n",
        "    fig = px.line(\n",
        "        x=list(range(len(results))),\n",
        "        y=results,\n",
        "        labels={'x': 'Evaluation Step', 'y': 'Average Distance'},\n",
        "        title='Average Distance over Evaluation Steps'\n",
        "    )\n",
        "    fig.show()\n",
        "\n",
        "def create_plotly_route_fig(env):\n",
        "    fig = go.Figure()\n",
        "\n",
        "    # Colors for different routes\n",
        "    colors = ['blue', 'orange', 'green']\n",
        "\n",
        "    # Add routes to the plot\n",
        "    for i, route in enumerate(env.routes):\n",
        "        route = np.array(route)\n",
        "        x_coords = env.VRP[route, 0]\n",
        "        y_coords = env.VRP[route, 1]\n",
        "        route_label = f'Route {i+1}'\n",
        "\n",
        "        fig.add_trace(go.Scatter(\n",
        "            x=x_coords, y=y_coords,\n",
        "            mode='lines+markers+text',\n",
        "            text=[str(j) for j in range(len(route))],\n",
        "            line=dict(color=colors[i], width=2),\n",
        "            marker=dict(size=8),\n",
        "            name=route_label\n",
        "        ))\n",
        "\n",
        "        # Add distance annotations\n",
        "        for j in range(1, len(route)):\n",
        "            x0, y0 = x_coords[j-1], y_coords[j-1]\n",
        "            x1, y1 = x_coords[j], y_coords[j]\n",
        "            mid_x, mid_y = (x0 + x1) / 2, (y0 + y1) / 2\n",
        "            distance = np.sqrt((x1 - x0) ** 2 + (y1 - y0) ** 2)\n",
        "            fig.add_trace(go.Scatter(\n",
        "                x=[mid_x], y=[mid_y],\n",
        "                text=[f'{distance:.2f}'],\n",
        "                mode='text',\n",
        "                showlegend=False\n",
        "            ))\n",
        "\n",
        "    # Plot the depots\n",
        "    fig.add_trace(go.Scatter(\n",
        "        x=env.VRP[:, 0], y=env.VRP[:, 1],\n",
        "        mode='markers',\n",
        "        marker=dict(size=10, color='red'),\n",
        "        name='Depots'\n",
        "    ))\n",
        "\n",
        "    fig.update_layout(\n",
        "        title='Vehicle Routes with Visit Order and Distances',\n",
        "        xaxis_title='X Coordinate',\n",
        "        yaxis_title='Y Coordinate',\n",
        "        legend=dict(x=1, y=1),\n",
        "        margin=dict(l=0, r=0, t=40, b=0),\n",
        "        template='plotly_white'\n",
        "    )\n",
        "\n",
        "    return fig\n",
        "\n",
        "# Print results mean\n",
        "print(results.mean())\n",
        "\n",
        "# Visualize using Plotly for routes\n",
        "fig_routes = create_plotly_route_fig(env)\n",
        "fig_routes.show()\n",
        "\n",
        "# Visualize using Plotly for average distances\n",
        "plot_average_distances(results)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "N2ufPTVKACNn",
        "outputId": "8beaf533-9852-4eac-d239-ebb00cc56e38"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------------\n",
            "Average Distance over the Evaluation Step: 4.127551\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "Average Distance over the Evaluation Step: 5.307200\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "Average Distance over the Evaluation Step: 4.922559\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "Average Distance over the Evaluation Step: 5.182442\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "Average Distance over the Evaluation Step: 6.792554\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "Average Distance over the Evaluation Step: 6.568832\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "Average Distance over the Evaluation Step: 5.543526\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "Average Distance over the Evaluation Step: 7.053209\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "Average Distance over the Evaluation Step: 5.997534\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "Average Distance over the Evaluation Step: 3.942214\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "Average Distance over the Evaluation Step: 6.471125\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "Average Distance over the Evaluation Step: 8.292821\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "Average Distance over the Evaluation Step: 5.331789\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "Average Distance over the Evaluation Step: 5.479321\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "Average Distance over the Evaluation Step: 5.594334\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "Average Distance over the Evaluation Step: 4.980261\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "Average Distance over the Evaluation Step: 6.202824\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "Average Distance over the Evaluation Step: 6.788422\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "Average Distance over the Evaluation Step: 5.713713\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "Average Distance over the Evaluation Step: 5.661250\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "Average Distance over the Evaluation Step: 5.256517\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "Average Distance over the Evaluation Step: 5.307535\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "Average Distance over the Evaluation Step: 5.633437\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "Average Distance over the Evaluation Step: 6.227948\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "Average Distance over the Evaluation Step: 4.960348\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "Average Distance over the Evaluation Step: 5.878776\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "Average Distance over the Evaluation Step: 4.730603\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "Average Distance over the Evaluation Step: 7.148236\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "Average Distance over the Evaluation Step: 6.237148\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "Average Distance over the Evaluation Step: 5.780657\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "Average Distance over the Evaluation Step: 5.510284\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "Average Distance over the Evaluation Step: 4.593409\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "Average Distance over the Evaluation Step: 4.650190\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "Average Distance over the Evaluation Step: 5.786819\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "Average Distance over the Evaluation Step: 6.347341\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "Average Distance over the Evaluation Step: 6.254047\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "Average Distance over the Evaluation Step: 6.803577\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "Average Distance over the Evaluation Step: 6.257429\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "Average Distance over the Evaluation Step: 3.737003\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "Average Distance over the Evaluation Step: 6.205194\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "Average Distance over the Evaluation Step: 6.070299\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "Average Distance over the Evaluation Step: 6.431955\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "Average Distance over the Evaluation Step: 9.053719\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "Average Distance over the Evaluation Step: 6.553674\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "Average Distance over the Evaluation Step: 4.940273\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "Average Distance over the Evaluation Step: 6.718924\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "Average Distance over the Evaluation Step: 5.789656\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "Average Distance over the Evaluation Step: 6.345879\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "Average Distance over the Evaluation Step: 5.029113\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "Average Distance over the Evaluation Step: 5.864960\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "Average Distance over the Evaluation Step: 4.428060\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "Average Distance over the Evaluation Step: 4.814490\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "Average Distance over the Evaluation Step: 6.272793\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "Average Distance over the Evaluation Step: 5.453808\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "Average Distance over the Evaluation Step: 5.806566\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "Average Distance over the Evaluation Step: 6.723195\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "Average Distance over the Evaluation Step: 5.604926\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "Average Distance over the Evaluation Step: 7.698193\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "Average Distance over the Evaluation Step: 5.614591\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "Average Distance over the Evaluation Step: 6.130279\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "Average Distance over the Evaluation Step: 5.950417\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "Average Distance over the Evaluation Step: 6.154713\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "Average Distance over the Evaluation Step: 6.837718\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "Average Distance over the Evaluation Step: 4.711889\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "Average Distance over the Evaluation Step: 3.753902\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "Average Distance over the Evaluation Step: 5.684487\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "Average Distance over the Evaluation Step: 4.910517\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "Average Distance over the Evaluation Step: 6.210938\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "Average Distance over the Evaluation Step: 6.124663\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "Average Distance over the Evaluation Step: 6.370397\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "Average Distance over the Evaluation Step: 4.899481\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "Average Distance over the Evaluation Step: 7.879836\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "Average Distance over the Evaluation Step: 5.621081\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "Average Distance over the Evaluation Step: 4.448367\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "Average Distance over the Evaluation Step: 6.982238\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "Average Distance over the Evaluation Step: 6.430597\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "Average Distance over the Evaluation Step: 6.124262\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "Average Distance over the Evaluation Step: 5.880881\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "Average Distance over the Evaluation Step: 5.819498\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "Average Distance over the Evaluation Step: 5.568396\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "Average Distance over the Evaluation Step: 6.633248\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "Average Distance over the Evaluation Step: 6.599458\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "Average Distance over the Evaluation Step: 5.648273\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "Average Distance over the Evaluation Step: 5.503702\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "Average Distance over the Evaluation Step: 5.725216\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "Average Distance over the Evaluation Step: 3.890804\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "Average Distance over the Evaluation Step: 5.944713\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "Average Distance over the Evaluation Step: 4.541044\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "Average Distance over the Evaluation Step: 5.409186\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "Average Distance over the Evaluation Step: 5.404035\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "Average Distance over the Evaluation Step: 4.122465\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "Average Distance over the Evaluation Step: 6.788680\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "Average Distance over the Evaluation Step: 5.698380\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "Average Distance over the Evaluation Step: 5.852298\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "Average Distance over the Evaluation Step: 4.478777\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "Average Distance over the Evaluation Step: 6.323532\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "Average Distance over the Evaluation Step: 6.777121\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "Average Distance over the Evaluation Step: 4.672704\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "Average Distance over the Evaluation Step: 5.424570\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "Average Distance over the Evaluation Step: 6.471220\n",
            "---------------------------------------\n",
            "[4.12755116 5.30719958 4.92255914 5.18244176 6.79255373 6.56883184\n",
            " 5.54352613 7.05320879 5.99753444 3.94221378 6.47112494 8.29282092\n",
            " 5.3317893  5.47932106 5.59433427 4.9802612  6.20282427 6.78842218\n",
            " 5.7137131  5.66124969 5.25651741 5.30753539 5.63343696 6.22794818\n",
            " 4.96034802 5.87877617 4.73060337 7.14823564 6.23714793 5.78065716\n",
            " 5.5102838  4.59340945 4.65018985 5.7868189  6.34734081 6.25404725\n",
            " 6.80357729 6.25742901 3.73700337 6.20519431 6.07029896 6.43195489\n",
            " 9.05371942 6.55367373 4.94027285 6.71892438 5.78965553 6.34587878\n",
            " 5.02911304 5.86495987 4.42805969 4.81448963 6.27279267 5.45380768\n",
            " 5.80656625 6.72319508 5.60492591 7.69819337 5.61459142 6.13027877\n",
            " 5.95041696 6.15471278 6.83771815 4.71188896 3.75390236 5.68448733\n",
            " 4.91051723 6.21093753 6.12466299 6.37039655 4.89948094 7.87983598\n",
            " 5.62108082 4.44836681 6.98223751 6.43059663 6.12426194 5.88088062\n",
            " 5.81949786 5.56839567 6.63324766 6.59945807 5.64827324 5.50370192\n",
            " 5.72521621 3.89080407 5.94471287 4.54104357 5.40918615 5.40403451\n",
            " 4.12246465 6.78868008 5.69838026 5.85229794 4.47877715 6.32353155\n",
            " 6.77712137 4.67270441 5.42456957 6.47122038]\n",
            "5.788790346880125\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.24.1.min.js\"></script>                <div id=\"70542c88-4721-414a-8595-24c7893e79cf\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"70542c88-4721-414a-8595-24c7893e79cf\")) {                    Plotly.newPlot(                        \"70542c88-4721-414a-8595-24c7893e79cf\",                        [{\"line\":{\"color\":\"blue\",\"width\":2},\"marker\":{\"size\":8},\"mode\":\"lines+markers+text\",\"name\":\"Route 1\",\"text\":[\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\"],\"x\":[0.6722785586307918,0.031446387626298145,0.5656174196105306,0.006825733043655191,0.29762249870394397,0.04669572050695325,0.6722785586307918],\"y\":[0.7467671009942997,0.9289483920955792,0.97395629682896,0.2116867855893635,0.5244147153752401,0.09361309329775258,0.7467671009942997],\"type\":\"scatter\"},{\"mode\":\"text\",\"showlegend\":false,\"text\":[\"0.67\"],\"x\":[0.35186247312854496],\"y\":[0.8378577465449395],\"type\":\"scatter\"},{\"mode\":\"text\",\"showlegend\":false,\"text\":[\"0.54\"],\"x\":[0.29853190361841436],\"y\":[0.9514523444622696],\"type\":\"scatter\"},{\"mode\":\"text\",\"showlegend\":false,\"text\":[\"0.95\"],\"x\":[0.2862215763270929],\"y\":[0.5928215412091618],\"type\":\"scatter\"},{\"mode\":\"text\",\"showlegend\":false,\"text\":[\"0.43\"],\"x\":[0.15222411587379958],\"y\":[0.3680507504823018],\"type\":\"scatter\"},{\"mode\":\"text\",\"showlegend\":false,\"text\":[\"0.50\"],\"x\":[0.1721591096054486],\"y\":[0.30901390433649634],\"type\":\"scatter\"},{\"mode\":\"text\",\"showlegend\":false,\"text\":[\"0.90\"],\"x\":[0.3594871395688725],\"y\":[0.4201900971460261],\"type\":\"scatter\"},{\"line\":{\"color\":\"orange\",\"width\":2},\"marker\":{\"size\":8},\"mode\":\"lines+markers+text\",\"name\":\"Route 2\",\"text\":[\"0\",\"1\",\"2\",\"3\",\"4\",\"5\"],\"x\":[0.6722785586307918,0.4880783992405837,0.7697930281899398,0.9906273994707961,0.8254951740358963,0.6722785586307918],\"y\":[0.7467671009942997,0.37743893633496517,0.5543457845906854,0.813308412782129,0.4941474522409759,0.7467671009942997],\"type\":\"scatter\"},{\"mode\":\"text\",\"showlegend\":false,\"text\":[\"0.41\"],\"x\":[0.5801784789356877],\"y\":[0.5621030186646324],\"type\":\"scatter\"},{\"mode\":\"text\",\"showlegend\":false,\"text\":[\"0.33\"],\"x\":[0.6289357137152618],\"y\":[0.4658923604628253],\"type\":\"scatter\"},{\"mode\":\"text\",\"showlegend\":false,\"text\":[\"0.34\"],\"x\":[0.880210213830368],\"y\":[0.6838270986864072],\"type\":\"scatter\"},{\"mode\":\"text\",\"showlegend\":false,\"text\":[\"0.36\"],\"x\":[0.9080612867533462],\"y\":[0.6537279325115524],\"type\":\"scatter\"},{\"mode\":\"text\",\"showlegend\":false,\"text\":[\"0.30\"],\"x\":[0.7488868663333441],\"y\":[0.6204572766176377],\"type\":\"scatter\"},{\"line\":{\"color\":\"green\",\"width\":2},\"marker\":{\"size\":8},\"mode\":\"lines+markers+text\",\"name\":\"Route 3\",\"text\":[\"0\",\"1\",\"2\"],\"x\":[0.6722785586307918,0.8080499633648477,0.6722785586307918],\"y\":[0.7467671009942997,0.3954540443458203,0.7467671009942997],\"type\":\"scatter\"},{\"mode\":\"text\",\"showlegend\":false,\"text\":[\"0.38\"],\"x\":[0.7401642609978197],\"y\":[0.57111057267006],\"type\":\"scatter\"},{\"mode\":\"text\",\"showlegend\":false,\"text\":[\"0.38\"],\"x\":[0.7401642609978197],\"y\":[0.57111057267006],\"type\":\"scatter\"},{\"marker\":{\"color\":\"red\",\"size\":10},\"mode\":\"markers\",\"name\":\"Depots\",\"x\":[0.6722785586307918,0.4880783992405837,0.8254951740358963,0.031446387626298145,0.8080499633648477,0.5656174196105306,0.29762249870394397,0.04669572050695325,0.9906273994707961,0.006825733043655191,0.7697930281899398],\"y\":[0.7467671009942997,0.37743893633496517,0.4941474522409759,0.9289483920955792,0.3954540443458203,0.97395629682896,0.5244147153752401,0.09361309329775258,0.813308412782129,0.2116867855893635,0.5543457845906854],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"white\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"#C8D4E3\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"white\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"radialaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"yaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"zaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"caxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2}}},\"legend\":{\"x\":1,\"y\":1},\"margin\":{\"l\":0,\"r\":0,\"t\":40,\"b\":0},\"title\":{\"text\":\"Vehicle Routes with Visit Order and Distances\"},\"xaxis\":{\"title\":{\"text\":\"X Coordinate\"}},\"yaxis\":{\"title\":{\"text\":\"Y Coordinate\"}}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('70542c88-4721-414a-8595-24c7893e79cf');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.24.1.min.js\"></script>                <div id=\"f9d5654e-7a99-40cd-b125-d9c0acc6f905\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"f9d5654e-7a99-40cd-b125-d9c0acc6f905\")) {                    Plotly.newPlot(                        \"f9d5654e-7a99-40cd-b125-d9c0acc6f905\",                        [{\"hovertemplate\":\"Evaluation Step=%{x}\\u003cbr\\u003eAverage Distance=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"\",\"line\":{\"color\":\"#636efa\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99],\"xaxis\":\"x\",\"y\":[4.127551164835507,5.307199581335342,4.922559140342045,5.18244176230572,6.792553726826964,6.5688318357864315,5.543526126399192,7.053208788706556,5.9975344418503,3.9422137805650923,6.471124936868317,8.292820919045441,5.331789301603126,5.479321061967074,5.59433427227454,4.980261199834567,6.202824265342922,6.788422179923573,5.713713096506998,5.661249687478541,5.256517410651584,5.307535394760136,5.633436963459586,6.227948182071757,4.960348015793577,5.878776165731212,4.730603374494741,7.148235642108765,6.237147926852938,5.78065715633553,5.51028379680904,4.593409448864678,4.650189851901524,5.786818895435218,6.347340808830584,6.254047247263342,6.803577293658174,6.25742900683721,3.7370033662374498,6.205194311501576,6.07029896095304,6.431954894277631,9.053719420475012,6.553673727936988,4.940272845872508,6.718924381739142,5.7896555304305375,6.345878784552579,5.029113038401028,5.864959871516996,4.428059686247375,4.814489630515783,6.272792666399779,5.453807678099973,5.806566250183225,6.72319507731673,5.604925907358517,7.698193373053391,5.614591415219779,6.130278774839693,5.950416955015057,6.154712782641619,6.837718151776792,4.71188896215363,3.753902360851711,5.684487325468352,4.910517225032618,6.210937534068935,6.124662985251472,6.370396548281139,4.899480937046183,7.87983598339161,5.621080817483676,4.448366814945022,6.982237507629079,6.430596633351265,6.124261939472393,5.88088062120306,5.819497857042246,5.568395672624712,6.633247657765775,6.599458068410121,5.648273237728251,5.503701916710083,5.725216208061184,3.8908040651151325,5.9447128706911805,4.541043572000847,5.409186154337517,5.404034509289779,4.122464654366742,6.788680079075586,5.698380260172129,5.852297944021922,4.478777152493644,6.323531554692622,6.777121374768474,4.6727044067583785,5.424569566885135,6.471220375150885],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Evaluation Step\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Average Distance\"}},\"legend\":{\"tracegroupgap\":0},\"title\":{\"text\":\"Average Distance over Evaluation Steps\"}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('f9d5654e-7a99-40cd-b125-d9c0acc6f905');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "KERNEL_DISPLAY_NAME",
      "language": "python",
      "name": "environment_name"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}